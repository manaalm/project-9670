{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Mechanism Verification\n",
    "\n",
    "This notebook quantifies the \"spurious vs robust\" reliance for each trained model.\n",
    "\n",
    "## Metrics computed:\n",
    "1. **OOD Drop**: Acc(ID) - Acc(OOD)\n",
    "2. **Counterfactual Patch Sensitivity**:\n",
    "   - Accuracy change when patch color is swapped\n",
    "   - Mean change in true-class logit\n",
    "3. **Spurious Reliance Score (SRS)**: Combined metric\n",
    "\n",
    "## Spurious Reliance Score (SRS) Formula:\n",
    "```\n",
    "SRS = 0.4 * OOD_drop + 0.3 * CF_accuracy_drop + 0.3 * CF_flip_rate\n",
    "```\n",
    "Where:\n",
    "- OOD_drop = ID_accuracy - OOD_accuracy\n",
    "- CF_accuracy_drop = Original_accuracy - Counterfactual_accuracy\n",
    "- CF_flip_rate = Fraction of correct predictions that flip when patch is swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    create_env_a_dataset,\n",
    "    create_no_patch_dataset,\n",
    "    SpuriousPatchDataset,\n",
    "    CounterfactualPatchDataset,\n",
    "    get_transforms,\n",
    "    DATA_DIR,\n",
    ")\n",
    "from src.models import create_model\n",
    "from src.train import load_model, evaluate_model\n",
    "from src.metrics import (\n",
    "    compute_ood_drop,\n",
    "    compute_patch_sensitivity,\n",
    "    compute_spurious_reliance_score,\n",
    "    compute_class_wise_accuracy,\n",
    ")\n",
    "from src.plotting import plot_spurious_reliance_comparison, save_figure\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 4 models\n",
    "model_names = ['A1', 'A2', 'R1', 'R2']\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\\n\"\n",
    "                               f\"Please run 02_train_models.ipynb first.\")\n",
    "    \n",
    "    model = create_model(config)\n",
    "    model = load_model(model, checkpoint_path, device)\n",
    "    models[name] = model\n",
    "    print(f\"Loaded model {name} from {checkpoint_path}\")\n",
    "\n",
    "print(f\"\\nAll {len(models)} models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets\n",
    "test_id = create_env_a_dataset(train=False, config=config)  # ID test (with aligned patches)\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)  # OOD test (no patches)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "id_loader = DataLoader(test_id, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "ood_loader = DataLoader(test_ood, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Test datasets:\")\n",
    "print(f\"  ID test (Env A): {len(test_id)} samples\")\n",
    "print(f\"  OOD test (No patch): {len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counterfactual dataset for patch sensitivity analysis\n",
    "# We use the ID test set as the base\n",
    "cf_dataset = CounterfactualPatchDataset(\n",
    "    base_dataset=test_id,\n",
    "    swap_mode='random_wrong',\n",
    ")\n",
    "\n",
    "print(f\"Counterfactual dataset: {len(cf_dataset)} samples\")\n",
    "print(\"  Each sample provides: (original_img, label, counterfactual_img)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Basic Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ID and OOD accuracy for all models\n",
    "print(\"Computing ID and OOD accuracy...\\n\")\n",
    "\n",
    "accuracy_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    _, id_acc = evaluate_model(model, id_loader, device)\n",
    "    _, ood_acc = evaluate_model(model, ood_loader, device)\n",
    "    ood_drop = compute_ood_drop(id_acc, ood_acc)\n",
    "    \n",
    "    accuracy_results[name] = {\n",
    "        'id_acc': id_acc,\n",
    "        'ood_acc': ood_acc,\n",
    "        'ood_drop': ood_drop,\n",
    "    }\n",
    "    \n",
    "    print(f\"Model {name}:\")\n",
    "    print(f\"  ID Accuracy:  {id_acc*100:.2f}%\")\n",
    "    print(f\"  OOD Accuracy: {ood_acc*100:.2f}%\")\n",
    "    print(f\"  OOD Drop:     {ood_drop*100:+.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Counterfactual Patch Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute patch sensitivity for all models\n",
    "print(\"Computing counterfactual patch sensitivity...\\n\")\n",
    "\n",
    "sensitivity_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nAnalyzing Model {name}...\")\n",
    "    sensitivity = compute_patch_sensitivity(\n",
    "        model, cf_dataset, device,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    sensitivity_results[name] = sensitivity\n",
    "    \n",
    "    print(f\"  Original Accuracy:      {sensitivity['original_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Counterfactual Accuracy: {sensitivity['counterfactual_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Accuracy Drop:          {sensitivity['accuracy_drop']*100:+.2f}%\")\n",
    "    print(f\"  Prediction Flip Rate:   {sensitivity['prediction_flip_rate']*100:.2f}%\")\n",
    "    print(f\"  Mean Logit Change:      {sensitivity['mean_logit_change']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Spurious Reliance Score (SRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full SRS for all models\n",
    "print(\"Computing Spurious Reliance Score (SRS)...\\n\")\n",
    "\n",
    "srs_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel {name}:\")\n",
    "    srs = compute_spurious_reliance_score(\n",
    "        model, id_loader, ood_loader, cf_dataset, device\n",
    "    )\n",
    "    srs_results[name] = srs\n",
    "    \n",
    "    print(f\"  Spurious Reliance Score: {srs['spurious_reliance_score']:.4f}\")\n",
    "    print(f\"  Components:\")\n",
    "    print(f\"    - OOD Drop:        {srs['ood_drop']*100:.2f}% (weight: 0.4)\")\n",
    "    print(f\"    - CF Acc Drop:     {srs['cf_accuracy_drop']*100:.2f}% (weight: 0.3)\")\n",
    "    print(f\"    - CF Flip Rate:    {srs['cf_flip_rate']*100:.2f}% (weight: 0.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SRS comparison\n",
    "fig = plot_spurious_reliance_comparison(\n",
    "    srs_results,\n",
    "    title='Spurious Reliance Metrics by Model',\n",
    "    save_name='spurious_reliance_comparison'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "model_names = list(srs_results.keys())\n",
    "x = np.arange(len(model_names))\n",
    "colors = ['#e74c3c', '#e67e22', '#3498db', '#2ecc71']\n",
    "\n",
    "# 1. SRS Score\n",
    "srs_values = [srs_results[m]['spurious_reliance_score'] for m in model_names]\n",
    "bars = axes[0, 0].bar(x, srs_values, color=colors)\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(model_names)\n",
    "axes[0, 0].set_ylabel('SRS')\n",
    "axes[0, 0].set_title('Spurious Reliance Score (SRS)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, srs_values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. ID vs OOD Accuracy\n",
    "id_accs = [srs_results[m]['id_accuracy']*100 for m in model_names]\n",
    "ood_accs = [srs_results[m]['ood_accuracy']*100 for m in model_names]\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, id_accs, width, label='ID Acc', color='steelblue')\n",
    "axes[0, 1].bar(x + width/2, ood_accs, width, label='OOD Acc', color='coral')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(model_names)\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('ID vs OOD Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Counterfactual Accuracy Drop\n",
    "cf_drops = [srs_results[m]['cf_accuracy_drop']*100 for m in model_names]\n",
    "bars = axes[1, 0].bar(x, cf_drops, color=colors)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(model_names)\n",
    "axes[1, 0].set_ylabel('Accuracy Drop (%)')\n",
    "axes[1, 0].set_title('Counterfactual Patch Accuracy Drop')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Prediction Flip Rate\n",
    "flip_rates = [srs_results[m]['cf_flip_rate']*100 for m in model_names]\n",
    "bars = axes[1, 1].bar(x, flip_rates, color=colors)\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(model_names)\n",
    "axes[1, 1].set_ylabel('Flip Rate (%)')\n",
    "axes[1, 1].set_title('Prediction Flip Rate (when patch swapped)')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Mechanism Verification: Spurious vs Robust Models', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'mechanism_verification_detailed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"MECHANISM VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\n{'Model':<8} {'Type':<10} {'ID Acc':<10} {'OOD Acc':<10} {'OOD Drop':<10} \"\n",
    "      f\"{'CF Drop':<10} {'Flip Rate':<10} {'SRS':<8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name in model_names:\n",
    "    model_type = \"Spurious\" if name.startswith('A') else \"Robust\"\n",
    "    srs = srs_results[name]\n",
    "    print(f\"{name:<8} {model_type:<10} {srs['id_accuracy']*100:>6.2f}%   \"\n",
    "          f\"{srs['ood_accuracy']*100:>6.2f}%   {srs['ood_drop']*100:>+6.2f}%   \"\n",
    "          f\"{srs['cf_accuracy_drop']*100:>6.2f}%   {srs['cf_flip_rate']*100:>6.2f}%   \"\n",
    "          f\"{srs['spurious_reliance_score']:.4f}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute group statistics\n",
    "spurious_models = ['A1', 'A2']\n",
    "robust_models = ['R1', 'R2']\n",
    "\n",
    "spurious_avg_srs = np.mean([srs_results[m]['spurious_reliance_score'] for m in spurious_models])\n",
    "robust_avg_srs = np.mean([srs_results[m]['spurious_reliance_score'] for m in robust_models])\n",
    "\n",
    "spurious_avg_drop = np.mean([srs_results[m]['ood_drop'] for m in spurious_models])\n",
    "robust_avg_drop = np.mean([srs_results[m]['ood_drop'] for m in robust_models])\n",
    "\n",
    "print(\"\\nGroup Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Spurious Models (A1, A2):\")\n",
    "print(f\"  Average SRS:      {spurious_avg_srs:.4f}\")\n",
    "print(f\"  Average OOD Drop: {spurious_avg_drop*100:.2f}%\")\n",
    "print(f\"\\nRobust Models (R1, R2):\")\n",
    "print(f\"  Average SRS:      {robust_avg_srs:.4f}\")\n",
    "print(f\"  Average OOD Drop: {robust_avg_drop*100:.2f}%\")\n",
    "print(f\"\\nSRS Ratio (Spurious/Robust): {spurious_avg_srs/robust_avg_srs:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification checks\n",
    "print(\"\\nVerification Checks:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check 1: Spurious models should have higher SRS\n",
    "if spurious_avg_srs > robust_avg_srs:\n",
    "    print(\"[PASS] Spurious models have higher SRS than robust models\")\n",
    "else:\n",
    "    print(\"[FAIL] Expected spurious models to have higher SRS\")\n",
    "\n",
    "# Check 2: Spurious models should have larger OOD drop\n",
    "if spurious_avg_drop > robust_avg_drop:\n",
    "    print(\"[PASS] Spurious models have larger OOD accuracy drop\")\n",
    "else:\n",
    "    print(\"[FAIL] Expected spurious models to have larger OOD drop\")\n",
    "\n",
    "# Check 3: All spurious models should have SRS > 0.1 (reasonable threshold)\n",
    "all_spurious_high_srs = all(srs_results[m]['spurious_reliance_score'] > 0.05 for m in spurious_models)\n",
    "if all_spurious_high_srs:\n",
    "    print(\"[PASS] All spurious models have SRS > 0.05\")\n",
    "else:\n",
    "    print(\"[WARN] Some spurious models have low SRS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to JSON\n",
    "mechanism_results = {\n",
    "    'srs_results': {k: {kk: float(vv) for kk, vv in v.items()} \n",
    "                   for k, v in srs_results.items()},\n",
    "    'group_statistics': {\n",
    "        'spurious_avg_srs': float(spurious_avg_srs),\n",
    "        'robust_avg_srs': float(robust_avg_srs),\n",
    "        'spurious_avg_ood_drop': float(spurious_avg_drop),\n",
    "        'robust_avg_ood_drop': float(robust_avg_drop),\n",
    "        'srs_ratio': float(spurious_avg_srs / robust_avg_srs) if robust_avg_srs > 0 else float('inf'),\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = METRICS_DIR / 'mechanism_verification.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(mechanism_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MECHANISM VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Key Findings:\n",
    "\n",
    "1. Spurious Reliance Score (SRS):\n",
    "   - Spurious models (A1, A2): Average SRS = {spurious_avg_srs:.4f}\n",
    "   - Robust models (R1, R2):   Average SRS = {robust_avg_srs:.4f}\n",
    "   - Ratio: {spurious_avg_srs/robust_avg_srs:.2f}x higher for spurious models\n",
    "\n",
    "2. OOD Accuracy Drop:\n",
    "   - Spurious models: {spurious_avg_drop*100:.1f}% average drop\n",
    "   - Robust models:   {robust_avg_drop*100:.1f}% average drop\n",
    "\n",
    "3. Interpretation:\n",
    "   - Spurious models (A1, A2) rely heavily on the colored patch\n",
    "   - When the patch is removed (OOD) or swapped (CF), accuracy drops significantly\n",
    "   - Robust models (R1, R2) learned more content-based features\n",
    "   - They are less sensitive to patch manipulation\n",
    "\n",
    "SRS Formula:\n",
    "   SRS = 0.4 * OOD_drop + 0.3 * CF_accuracy_drop + 0.3 * flip_rate\n",
    "\n",
    "Files saved:\n",
    "   - {METRICS_DIR / 'mechanism_verification.json'}\n",
    "   - {FIGURES_DIR / 'spurious_reliance_comparison.png'}\n",
    "   - {FIGURES_DIR / 'mechanism_verification_detailed.png'}\n",
    "\n",
    "Next: Run 04_rebasin_alignment.ipynb to perform weight matching.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
