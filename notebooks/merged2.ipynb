{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Setup and Configuration\n",
    "\n",
    "This notebook sets up the environment for the Git Re-Basin spurious features experiment.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Validates all dependencies are installed\n",
    "2. Defines the global CONFIG dictionary\n",
    "3. Sets deterministic seeds for reproducibility\n",
    "4. Creates necessary directories\n",
    "5. Verifies GPU availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Add src to path and validate imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate core dependencies\n",
    "import importlib\n",
    "\n",
    "dependencies = [\n",
    "    ('torch', 'PyTorch'),\n",
    "    ('torchvision', 'TorchVision'),\n",
    "    ('numpy', 'NumPy'),\n",
    "    ('scipy', 'SciPy'),\n",
    "    ('sklearn', 'Scikit-learn'),\n",
    "    ('matplotlib', 'Matplotlib'),\n",
    "    ('seaborn', 'Seaborn'),\n",
    "    ('tqdm', 'tqdm'),\n",
    "    ('PIL', 'Pillow'),\n",
    "]\n",
    "\n",
    "print(\"Checking dependencies...\\n\")\n",
    "all_ok = True\n",
    "\n",
    "for module_name, display_name in dependencies:\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"  [OK] {display_name}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"  [MISSING] {display_name}\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nAll dependencies are installed!\")\n",
    "else:\n",
    "    print(\"\\nSome dependencies are missing. Run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate src module imports\n",
    "print(\"Checking src modules...\\n\")\n",
    "\n",
    "src_modules = ['config', 'data', 'models', 'train', 'rebasin', 'interp', 'metrics', 'plotting']\n",
    "\n",
    "for module_name in src_modules:\n",
    "    try:\n",
    "        module = importlib.import_module(f'src.{module_name}')\n",
    "        print(f\"  [OK] src.{module_name}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  [ERROR] src.{module_name}: {e}\")\n",
    "\n",
    "print(\"\\nAll src modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import get_config, CONFIG, set_seed, get_device, setup_directories\n",
    "\n",
    "# Load configuration\n",
    "config = get_config()\n",
    "\n",
    "print(\"Global Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for section, values in config.items():\n",
    "    print(f\"\\n[{section}]\")\n",
    "    if isinstance(values, dict):\n",
    "        for key, val in values.items():\n",
    "            print(f\"  {key}: {val}\")\n",
    "    else:\n",
    "        print(f\"  {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Deterministic Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set global seed\n",
    "GLOBAL_SEED = config['seeds']['global']\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "print(f\"Global seed set to: {GLOBAL_SEED}\")\n",
    "print(f\"\\nModel seeds:\")\n",
    "print(f\"  Model A1 (spurious): {config['seeds']['model_A1']}\")\n",
    "print(f\"  Model A2 (spurious): {config['seeds']['model_A2']}\")\n",
    "print(f\"  Model R1 (robust):   {config['seeds']['model_R1']}\")\n",
    "print(f\"  Model R2 (robust):   {config['seeds']['model_R2']}\")\n",
    "\n",
    "# Verify determinism\n",
    "print(f\"\\nDeterminism settings:\")\n",
    "print(f\"  torch.backends.cudnn.deterministic: {torch.backends.cudnn.deterministic}\")\n",
    "print(f\"  torch.backends.cudnn.benchmark: {torch.backends.cudnn.benchmark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all necessary directories\n",
    "dirs = setup_directories()\n",
    "\n",
    "print(\"Directory structure:\")\n",
    "for name, path in dirs.items():\n",
    "    exists = \"[EXISTS]\" if path.exists() else \"[CREATED]\"\n",
    "    print(f\"  {exists} {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"\\nCUDA Details:\")\n",
    "    print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"  Memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "elif device.type == 'mps':\n",
    "    print(f\"\\nUsing Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    print(f\"\\nNo GPU available, using CPU. Training will be slower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that we can create a model and do a forward pass\n",
    "from src.models import create_model, count_parameters\n",
    "\n",
    "model = create_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Model sanity check:\")\n",
    "print(f\"  Model architecture: {config['model']['architecture']}\")\n",
    "print(f\"  Parameters: {count_parameters(model):,}\")\n",
    "print(f\"  Input shape: {dummy_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Forward pass: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "from src.data import create_env_a_dataset, create_no_patch_dataset\n",
    "\n",
    "print(\"Testing data loading...\")\n",
    "\n",
    "# This will download CIFAR-10 if not present\n",
    "env_a_train = create_env_a_dataset(train=True, config=config)\n",
    "env_a_test = create_env_a_dataset(train=False, config=config)\n",
    "no_patch_test = create_no_patch_dataset(train=False, config=config)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Env A train: {len(env_a_train)}\")\n",
    "print(f\"  Env A test (ID): {len(env_a_test)}\")\n",
    "print(f\"  No patch test (OOD): {len(no_patch_test)}\")\n",
    "\n",
    "# Verify alignment rate\n",
    "alignment_rate = env_a_train.get_alignment_rate()\n",
    "expected_rate = config['patch']['p_align_env_a']\n",
    "print(f\"\\nEnv A alignment rate: {alignment_rate:.3f} (expected: {expected_rate})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Environment:\n",
    "  - Device: {device}\n",
    "  - Global seed: {GLOBAL_SEED}\n",
    "  - All dependencies: OK\n",
    "  - All src modules: OK\n",
    "  - Directory structure: OK\n",
    "\n",
    "Configuration:\n",
    "  - Dataset: {config['data']['dataset']}\n",
    "  - Patch size: {config['patch']['size']}x{config['patch']['size']}\n",
    "  - Env A alignment: {config['patch']['p_align_env_a']}\n",
    "  - Env B alignment: {config['patch']['p_align_env_b']}\n",
    "  - Model: {config['model']['architecture']}\n",
    "  - Training epochs: {config['training']['num_epochs']}\n",
    "  - Batch size: {config['training']['batch_size']}\n",
    "\n",
    "Next steps:\n",
    "  1. Run 01_data_spurious_envs.ipynb to visualize datasets\n",
    "  2. Run 02_train_models.ipynb to train all 4 models\n",
    "  3. Continue with remaining notebooks in order\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config to results for reference\n",
    "import json\n",
    "from src.config import RESULTS_DIR\n",
    "\n",
    "config_path = RESULTS_DIR / 'config.json'\n",
    "\n",
    "# Convert config to JSON-serializable format\n",
    "config_json = {}\n",
    "for key, value in config.items():\n",
    "    if isinstance(value, dict):\n",
    "        config_json[key] = {}\n",
    "        for k, v in value.items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                config_json[key][k] = list(v)\n",
    "            else:\n",
    "                config_json[key][k] = v\n",
    "    else:\n",
    "        config_json[key] = value\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_json, f, indent=2)\n",
    "\n",
    "print(f\"Configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data: Spurious Environments\n",
    "\n",
    "This notebook implements and visualizes the CIFAR-10 environments with spurious colored patches.\n",
    "\n",
    "## Environments:\n",
    "- **Env A (Spurious Aligned)**: Patch color matches label with probability 0.95\n",
    "- **Env B (Spurious Flipped)**: Patch color matches label with probability 0.05\n",
    "- **No Patch (OOD)**: Clean CIFAR-10 without any patches\n",
    "\n",
    "## What this notebook does:\n",
    "1. Creates all environment datasets\n",
    "2. Visualizes sample images from each environment\n",
    "3. Verifies alignment rates\n",
    "4. Saves visualization figures to results/figures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Load configuration and set seeds\n",
    "from src.config import get_config, set_seed, get_device, FIGURES_DIR, CIFAR10_CLASSES\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Figures will be saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data import (\n",
    "    create_env_a_dataset,\n",
    "    create_env_b_dataset,\n",
    "    create_no_patch_dataset,\n",
    "    create_mixed_env_dataset,\n",
    "    denormalize,\n",
    "    get_sample_batch,\n",
    ")\n",
    "from src.plotting import plot_sample_grid, save_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all environment datasets\n",
    "print(\"Creating datasets (this may download CIFAR-10)...\\n\")\n",
    "\n",
    "# Training sets\n",
    "env_a_train = create_env_a_dataset(train=True, config=config)\n",
    "env_b_train = create_env_b_dataset(train=True, config=config)\n",
    "mixed_train = create_mixed_env_dataset(env_a_fraction=0.5, train=True, config=config)\n",
    "\n",
    "# Test sets\n",
    "env_a_test = create_env_a_dataset(train=False, config=config)\n",
    "env_b_test = create_env_b_dataset(train=False, config=config)\n",
    "no_patch_test = create_no_patch_dataset(train=False, config=config)\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Env A train: {len(env_a_train)}\")\n",
    "print(f\"  Env B train: {len(env_b_train)}\")\n",
    "print(f\"  Mixed train: {len(mixed_train)}\")\n",
    "print(f\"  Env A test (ID): {len(env_a_test)}\")\n",
    "print(f\"  Env B test: {len(env_b_test)}\")\n",
    "print(f\"  No patch test (OOD): {len(no_patch_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Alignment Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check alignment rates\n",
    "print(\"Alignment rate verification:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env_a_rate = env_a_train.get_alignment_rate()\n",
    "env_b_rate = env_b_train.get_alignment_rate()\n",
    "\n",
    "print(f\"\\nEnv A (spurious aligned):\")\n",
    "print(f\"  Expected alignment: {config['patch']['p_align_env_a']}\")\n",
    "print(f\"  Actual alignment:   {env_a_rate:.4f}\")\n",
    "print(f\"  Difference:         {abs(env_a_rate - config['patch']['p_align_env_a']):.4f}\")\n",
    "\n",
    "print(f\"\\nEnv B (spurious flipped):\")\n",
    "print(f\"  Expected alignment: {config['patch']['p_align_env_b']}\")\n",
    "print(f\"  Actual alignment:   {env_b_rate:.4f}\")\n",
    "print(f\"  Difference:         {abs(env_b_rate - config['patch']['p_align_env_b']):.4f}\")\n",
    "\n",
    "# Sanity check\n",
    "assert abs(env_a_rate - config['patch']['p_align_env_a']) < 0.02, \"Env A alignment rate too far from expected!\"\n",
    "assert abs(env_b_rate - config['patch']['p_align_env_b']) < 0.02, \"Env B alignment rate too far from expected!\"\n",
    "print(\"\\nSanity check PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Patch Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the color palette for each class\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "class_colors = config['patch']['class_colors']\n",
    "n_classes = len(class_colors)\n",
    "\n",
    "for i, (color, class_name) in enumerate(zip(class_colors, CIFAR10_CLASSES)):\n",
    "    # Normalize color to [0, 1]\n",
    "    norm_color = tuple(c / 255.0 for c in color)\n",
    "    rect = plt.Rectangle((i, 0), 1, 1, facecolor=norm_color, edgecolor='black')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i + 0.5, -0.2, class_name, ha='center', va='top', fontsize=10, rotation=45)\n",
    "    ax.text(i + 0.5, 0.5, f\"{color}\", ha='center', va='center', fontsize=8,\n",
    "            color='white' if sum(color) < 400 else 'black')\n",
    "\n",
    "ax.set_xlim(0, n_classes)\n",
    "ax.set_ylim(-0.5, 1)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Patch Color Palette by Class', fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'color_palette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Environment A (Spurious Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch from Env A\n",
    "images_a, labels_a = get_sample_batch(env_a_train, n_samples=16, seed=42)\n",
    "\n",
    "# Denormalize images for visualization\n",
    "images_a_vis = np.array([denormalize(img, config) for img in images_a])\n",
    "\n",
    "fig = plot_sample_grid(\n",
    "    images_a_vis,\n",
    "    labels_a.numpy(),\n",
    "    title=\"Environment A (Spurious Aligned, p=0.95)\",\n",
    "    nrow=4,\n",
    "    ncol=4,\n",
    "    save_name='env_a_samples'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: Check that patch colors mostly match labels\n",
    "print(\"Env A: Verifying patch-label alignment...\\n\")\n",
    "\n",
    "n_check = 100\n",
    "aligned = 0\n",
    "for i in range(n_check):\n",
    "    _, label = env_a_train.cifar[i]\n",
    "    patch_color = env_a_train.get_patch_color(i)\n",
    "    expected_color = class_colors[label]\n",
    "    if patch_color == expected_color:\n",
    "        aligned += 1\n",
    "\n",
    "print(f\"First {n_check} samples: {aligned}/{n_check} aligned ({aligned/n_check*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Environment B (Spurious Flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch from Env B\n",
    "images_b, labels_b = get_sample_batch(env_b_train, n_samples=16, seed=42)\n",
    "\n",
    "# Denormalize images for visualization\n",
    "images_b_vis = np.array([denormalize(img, config) for img in images_b])\n",
    "\n",
    "fig = plot_sample_grid(\n",
    "    images_b_vis,\n",
    "    labels_b.numpy(),\n",
    "    title=\"Environment B (Spurious Flipped, p=0.05)\",\n",
    "    nrow=4,\n",
    "    ncol=4,\n",
    "    save_name='env_b_samples'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: Check that patch colors mostly DON'T match labels\n",
    "print(\"Env B: Verifying patch-label misalignment...\\n\")\n",
    "\n",
    "n_check = 100\n",
    "aligned = 0\n",
    "for i in range(n_check):\n",
    "    _, label = env_b_train.cifar[i]\n",
    "    patch_color = env_b_train.get_patch_color(i)\n",
    "    expected_color = class_colors[label]\n",
    "    if patch_color == expected_color:\n",
    "        aligned += 1\n",
    "\n",
    "print(f\"First {n_check} samples: {aligned}/{n_check} aligned ({aligned/n_check*100:.1f}%)\")\n",
    "print(f\"Expected ~{config['patch']['p_align_env_b']*100}% alignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize No-Patch (OOD) Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch from No Patch dataset\n",
    "images_np, labels_np = get_sample_batch(no_patch_test, n_samples=16, seed=42)\n",
    "\n",
    "# Denormalize images for visualization\n",
    "images_np_vis = np.array([denormalize(img, config) for img in images_np])\n",
    "\n",
    "fig = plot_sample_grid(\n",
    "    images_np_vis,\n",
    "    labels_np.numpy(),\n",
    "    title=\"No Patch (OOD Test Set)\",\n",
    "    nrow=4,\n",
    "    ncol=4,\n",
    "    save_name='no_patch_samples'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison figure\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "\n",
    "# Get samples with same seed for comparison\n",
    "images_a, labels_a = get_sample_batch(env_a_train, n_samples=8, seed=123)\n",
    "images_b, labels_b = get_sample_batch(env_b_train, n_samples=8, seed=123)\n",
    "images_np, labels_np = get_sample_batch(no_patch_test, n_samples=8, seed=123)\n",
    "\n",
    "# Note: these are different images due to different datasets\n",
    "# But we use the same seed for consistent sampling within each dataset\n",
    "\n",
    "row_titles = ['Env A (aligned)', 'Env B (flipped)', 'No Patch (OOD)']\n",
    "all_images = [images_a, images_b, images_np]\n",
    "all_labels = [labels_a, labels_b, labels_np]\n",
    "\n",
    "for row, (images, labels, title) in enumerate(zip(all_images, all_labels, row_titles)):\n",
    "    images_vis = [denormalize(img, config) for img in images]\n",
    "    for col, (img, label) in enumerate(zip(images_vis, labels)):\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(CIFAR10_CLASSES[label], fontsize=9)\n",
    "    axes[row, 0].set_ylabel(title, fontsize=10, rotation=90, labelpad=40)\n",
    "\n",
    "plt.suptitle('Environment Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'environment_comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Class Distribution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify class distribution is balanced\n",
    "from collections import Counter\n",
    "\n",
    "def get_class_distribution(dataset):\n",
    "    labels = [dataset.cifar[i][1] for i in range(len(dataset))]\n",
    "    return Counter(labels)\n",
    "\n",
    "env_a_dist = get_class_distribution(env_a_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "classes = list(range(10))\n",
    "counts = [env_a_dist[c] for c in classes]\n",
    "\n",
    "bars = ax.bar(classes, counts, color='steelblue', edgecolor='black')\n",
    "ax.set_xticks(classes)\n",
    "ax.set_xticklabels(CIFAR10_CLASSES, rotation=45, ha='right')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('CIFAR-10 Class Distribution (Training Set)')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "            str(count), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'class_distribution')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal samples: {sum(counts)}\")\n",
    "print(f\"Expected per class: {sum(counts)//10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Environments created:\n",
    "\n",
    "1. Environment A (Spurious Aligned)\n",
    "   - Patch color matches true label with p={config['patch']['p_align_env_a']}\n",
    "   - Training: {len(env_a_train)} samples\n",
    "   - Test (ID): {len(env_a_test)} samples\n",
    "   - Actual alignment rate: {env_a_rate:.4f}\n",
    "\n",
    "2. Environment B (Spurious Flipped)\n",
    "   - Patch color matches true label with p={config['patch']['p_align_env_b']}\n",
    "   - Training: {len(env_b_train)} samples\n",
    "   - Actual alignment rate: {env_b_rate:.4f}\n",
    "\n",
    "3. No Patch (OOD Test)\n",
    "   - Clean CIFAR-10 without patches\n",
    "   - Test: {len(no_patch_test)} samples\n",
    "\n",
    "4. Mixed Environment (for robust training)\n",
    "   - 50% Env A + 50% Env B\n",
    "   - Training: {len(mixed_train)} samples\n",
    "\n",
    "Patch configuration:\n",
    "   - Size: {config['patch']['size']}x{config['patch']['size']} pixels\n",
    "   - Position: {config['patch']['position']}\n",
    "   - Colors: 10 distinct colors (one per class)\n",
    "\n",
    "Figures saved:\n",
    "   - {FIGURES_DIR / 'color_palette.png'}\n",
    "   - {FIGURES_DIR / 'env_a_samples.png'}\n",
    "   - {FIGURES_DIR / 'env_b_samples.png'}\n",
    "   - {FIGURES_DIR / 'no_patch_samples.png'}\n",
    "   - {FIGURES_DIR / 'environment_comparison.png'}\n",
    "   - {FIGURES_DIR / 'class_distribution.png'}\n",
    "\n",
    "Next: Run 02_train_models.ipynb to train the 4 models.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Train Models\n",
    "\n",
    "This notebook trains the 4 models needed for the experiment:\n",
    "\n",
    "1. **Model A1** (Spurious, seed 1): ERM on Env A only\n",
    "2. **Model A2** (Spurious, seed 2): ERM on Env A only\n",
    "3. **Model R1** (Robust, seed 1): ERM on mixed Env A + Env B\n",
    "4. **Model R2** (Robust, seed 2): ERM on mixed Env A + Env B\n",
    "\n",
    "## Expected behavior:\n",
    "- Spurious models (A1, A2) will achieve high ID accuracy but low OOD accuracy\n",
    "- Robust models (R1, R2) will have similar ID and OOD accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    create_env_a_dataset,\n",
    "    create_no_patch_dataset,\n",
    "    create_mixed_env_dataset,\n",
    "    get_dataloaders,\n",
    ")\n",
    "from src.models import create_model, count_parameters\n",
    "from src.train import Trainer, save_training_history\n",
    "from src.plotting import plot_training_curves, plot_multiple_training_curves, save_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets (same for all models)\n",
    "test_id = create_env_a_dataset(train=False, config=config)  # ID test\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)  # OOD test\n",
    "\n",
    "# Create training datasets\n",
    "train_spurious = create_env_a_dataset(train=True, config=config)  # For A1, A2\n",
    "train_robust = create_mixed_env_dataset(env_a_fraction=0.5, train=True, config=config)  # For R1, R2\n",
    "\n",
    "print(\"Datasets created:\")\n",
    "print(f\"  Spurious training (Env A): {len(train_spurious)} samples\")\n",
    "print(f\"  Robust training (Mixed): {len(train_robust)} samples\")\n",
    "print(f\"  ID test: {len(test_id)} samples\")\n",
    "print(f\"  OOD test: {len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "loaders_spurious = get_dataloaders(train_spurious, test_id, test_ood, config)\n",
    "loaders_robust = get_dataloaders(train_robust, test_id, test_ood, config)\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, seed, dataloaders, config, device):\n",
    "    \"\"\"\n",
    "    Train a single model and save checkpoint + history.\n",
    "    \n",
    "    Args:\n",
    "        name: Model name (e.g., 'A1', 'R1')\n",
    "        seed: Random seed for this model\n",
    "        dataloaders: Dictionary with 'train', 'test_id', 'test_ood' loaders\n",
    "        config: Configuration dictionary\n",
    "        device: Torch device\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Model {name} (seed={seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(model, device, config)\n",
    "    \n",
    "    # Train\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    history = trainer.train(\n",
    "        train_loader=dataloaders['train'],\n",
    "        test_id_loader=dataloaders['test_id'],\n",
    "        test_ood_loader=dataloaders['test_ood'],\n",
    "        num_epochs=config['training']['num_epochs'],\n",
    "        verbose=True,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history_path = METRICS_DIR / f\"history_{name}.json\"\n",
    "    save_training_history(history, history_path)\n",
    "    \n",
    "    print(f\"\\nModel {name} training complete!\")\n",
    "    print(f\"  Final ID accuracy: {history['id_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"  Final OOD accuracy: {history['ood_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"  Checkpoint saved to: {checkpoint_path}\")\n",
    "    print(f\"  History saved to: {history_path}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Spurious Model A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A1, history_A1 = train_model(\n",
    "    name='A1',\n",
    "    seed=config['seeds']['model_A1'],\n",
    "    dataloaders=loaders_spurious,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Spurious Model A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A2, history_A2 = train_model(\n",
    "    name='A2',\n",
    "    seed=config['seeds']['model_A2'],\n",
    "    dataloaders=loaders_spurious,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Robust Model R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_R1, history_R1 = train_model(\n",
    "    name='R1',\n",
    "    seed=config['seeds']['model_R1'],\n",
    "    dataloaders=loaders_robust,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Robust Model R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_R2, history_R2 = train_model(\n",
    "    name='R2',\n",
    "    seed=config['seeds']['model_R2'],\n",
    "    dataloaders=loaders_robust,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual training curves for each model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plot_training_curves(history_A1, title=\"Model A1 (Spurious, seed 1)\", save_name='training_A1')\n",
    "plt.show()\n",
    "\n",
    "fig = plot_training_curves(history_A2, title=\"Model A2 (Spurious, seed 2)\", save_name='training_A2')\n",
    "plt.show()\n",
    "\n",
    "fig = plot_training_curves(history_R1, title=\"Model R1 (Robust, seed 1)\", save_name='training_R1')\n",
    "plt.show()\n",
    "\n",
    "fig = plot_training_curves(history_R2, title=\"Model R2 (Robust, seed 2)\", save_name='training_R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plots\n",
    "histories = {\n",
    "    'A1 (Spurious)': history_A1,\n",
    "    'A2 (Spurious)': history_A2,\n",
    "    'R1 (Robust)': history_R1,\n",
    "    'R2 (Robust)': history_R2,\n",
    "}\n",
    "\n",
    "# ID accuracy comparison\n",
    "fig = plot_multiple_training_curves(\n",
    "    histories, \n",
    "    metric='id_acc', \n",
    "    title='ID Test Accuracy Comparison',\n",
    "    save_name='comparison_id_acc'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# OOD accuracy comparison\n",
    "fig = plot_multiple_training_curves(\n",
    "    histories, \n",
    "    metric='ood_acc', \n",
    "    title='OOD Test Accuracy Comparison',\n",
    "    save_name='comparison_ood_acc'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined comparison figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'A1': 'tab:red', 'A2': 'tab:orange', 'R1': 'tab:blue', 'R2': 'tab:green'}\n",
    "linestyles = {'A1': '-', 'A2': '--', 'R1': '-', 'R2': '--'}\n",
    "\n",
    "for name, history in [('A1', history_A1), ('A2', history_A2), ('R1', history_R1), ('R2', history_R2)]:\n",
    "    epochs = range(1, len(history['id_acc']) + 1)\n",
    "    \n",
    "    axes[0].plot(epochs, [x*100 for x in history['id_acc']], \n",
    "                 label=name, color=colors[name], linestyle=linestyles[name], linewidth=2)\n",
    "    axes[1].plot(epochs, [x*100 for x in history['ood_acc']], \n",
    "                 label=name, color=colors[name], linestyle=linestyles[name], linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('ID Test Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('OOD Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Progress: Spurious vs Robust Models', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'training_comparison_combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display summary statistics\n",
    "summary = {}\n",
    "\n",
    "for name, history in [('A1', history_A1), ('A2', history_A2), ('R1', history_R1), ('R2', history_R2)]:\n",
    "    final_id_acc = history['id_acc'][-1]\n",
    "    final_ood_acc = history['ood_acc'][-1]\n",
    "    ood_drop = final_id_acc - final_ood_acc\n",
    "    \n",
    "    summary[name] = {\n",
    "        'id_acc': final_id_acc,\n",
    "        'ood_acc': final_ood_acc,\n",
    "        'ood_drop': ood_drop,\n",
    "        'final_train_loss': history['train_loss'][-1],\n",
    "    }\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10} {'ID Acc':<12} {'OOD Acc':<12} {'OOD Drop':<12} {'Train Loss':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, stats in summary.items():\n",
    "    model_type = \"Spurious\" if name.startswith('A') else \"Robust\"\n",
    "    print(f\"{name} ({model_type[0]})   {stats['id_acc']*100:>6.2f}%      {stats['ood_acc']*100:>6.2f}%      \"\n",
    "          f\"{stats['ood_drop']*100:>+6.2f}%      {stats['final_train_loss']:.4f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify expected behavior\n",
    "print(\"\\nVerification:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Spurious models should have large OOD drop\n",
    "spurious_avg_drop = (summary['A1']['ood_drop'] + summary['A2']['ood_drop']) / 2\n",
    "robust_avg_drop = (summary['R1']['ood_drop'] + summary['R2']['ood_drop']) / 2\n",
    "\n",
    "print(f\"Average OOD drop (Spurious A1, A2): {spurious_avg_drop*100:.2f}%\")\n",
    "print(f\"Average OOD drop (Robust R1, R2):   {robust_avg_drop*100:.2f}%\")\n",
    "\n",
    "if spurious_avg_drop > robust_avg_drop + 0.1:\n",
    "    print(\"\\n[PASS] Spurious models show significantly larger OOD drop!\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] OOD drop difference is smaller than expected.\")\n",
    "    print(\"          This may affect the semantic barrier analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "summary_path = METRICS_DIR / 'training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Models trained:\n",
    "\n",
    "Spurious Models (trained on Env A only):\n",
    "  - A1: ID={summary['A1']['id_acc']*100:.1f}%, OOD={summary['A1']['ood_acc']*100:.1f}%, Drop={summary['A1']['ood_drop']*100:+.1f}%\n",
    "  - A2: ID={summary['A2']['id_acc']*100:.1f}%, OOD={summary['A2']['ood_acc']*100:.1f}%, Drop={summary['A2']['ood_drop']*100:+.1f}%\n",
    "\n",
    "Robust Models (trained on mixed Env A + B):\n",
    "  - R1: ID={summary['R1']['id_acc']*100:.1f}%, OOD={summary['R1']['ood_acc']*100:.1f}%, Drop={summary['R1']['ood_drop']*100:+.1f}%\n",
    "  - R2: ID={summary['R2']['id_acc']*100:.1f}%, OOD={summary['R2']['ood_acc']*100:.1f}%, Drop={summary['R2']['ood_drop']*100:+.1f}%\n",
    "\n",
    "Checkpoints saved:\n",
    "  - {CHECKPOINTS_DIR / 'model_A1.pt'}\n",
    "  - {CHECKPOINTS_DIR / 'model_A2.pt'}\n",
    "  - {CHECKPOINTS_DIR / 'model_R1.pt'}\n",
    "  - {CHECKPOINTS_DIR / 'model_R2.pt'}\n",
    "\n",
    "Training histories saved:\n",
    "  - {METRICS_DIR / 'history_A1.json'}\n",
    "  - {METRICS_DIR / 'history_A2.json'}\n",
    "  - {METRICS_DIR / 'history_R1.json'}\n",
    "  - {METRICS_DIR / 'history_R2.json'}\n",
    "\n",
    "Figures saved:\n",
    "  - Training curves for each model\n",
    "  - Comparison plots\n",
    "\n",
    "Next: Run 03_mechanism_verification.ipynb to quantify spurious reliance.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Mechanism Verification\n",
    "\n",
    "This notebook quantifies the \"spurious vs robust\" reliance for each trained model.\n",
    "\n",
    "## Metrics computed:\n",
    "1. **OOD Drop**: Acc(ID) - Acc(OOD)\n",
    "2. **Counterfactual Patch Sensitivity**:\n",
    "   - Accuracy change when patch color is swapped\n",
    "   - Mean change in true-class logit\n",
    "3. **Spurious Reliance Score (SRS)**: Combined metric\n",
    "\n",
    "## Spurious Reliance Score (SRS) Formula:\n",
    "```\n",
    "SRS = 0.4 * OOD_drop + 0.3 * CF_accuracy_drop + 0.3 * CF_flip_rate\n",
    "```\n",
    "Where:\n",
    "- OOD_drop = ID_accuracy - OOD_accuracy\n",
    "- CF_accuracy_drop = Original_accuracy - Counterfactual_accuracy\n",
    "- CF_flip_rate = Fraction of correct predictions that flip when patch is swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    create_env_a_dataset,\n",
    "    create_no_patch_dataset,\n",
    "    SpuriousPatchDataset,\n",
    "    CounterfactualPatchDataset,\n",
    "    get_transforms,\n",
    "    DATA_DIR,\n",
    ")\n",
    "from src.models import create_model\n",
    "from src.train import load_model, evaluate_model\n",
    "from src.metrics import (\n",
    "    compute_ood_drop,\n",
    "    compute_patch_sensitivity,\n",
    "    compute_spurious_reliance_score,\n",
    "    compute_class_wise_accuracy,\n",
    ")\n",
    "from src.plotting import plot_spurious_reliance_comparison, save_figure\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 4 models\n",
    "model_names = ['A1', 'A2', 'R1', 'R2']\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\\n\"\n",
    "                               f\"Please run 02_train_models.ipynb first.\")\n",
    "    \n",
    "    model = create_model(config)\n",
    "    model = load_model(model, checkpoint_path, device)\n",
    "    models[name] = model\n",
    "    print(f\"Loaded model {name} from {checkpoint_path}\")\n",
    "\n",
    "print(f\"\\nAll {len(models)} models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets\n",
    "test_id = create_env_a_dataset(train=False, config=config)  # ID test (with aligned patches)\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)  # OOD test (no patches)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "id_loader = DataLoader(test_id, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "ood_loader = DataLoader(test_ood, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Test datasets:\")\n",
    "print(f\"  ID test (Env A): {len(test_id)} samples\")\n",
    "print(f\"  OOD test (No patch): {len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counterfactual dataset for patch sensitivity analysis\n",
    "# We use the ID test set as the base\n",
    "cf_dataset = CounterfactualPatchDataset(\n",
    "    base_dataset=test_id,\n",
    "    swap_mode='random_wrong',\n",
    ")\n",
    "\n",
    "print(f\"Counterfactual dataset: {len(cf_dataset)} samples\")\n",
    "print(\"  Each sample provides: (original_img, label, counterfactual_img)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Basic Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ID and OOD accuracy for all models\n",
    "print(\"Computing ID and OOD accuracy...\\n\")\n",
    "\n",
    "accuracy_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    _, id_acc = evaluate_model(model, id_loader, device)\n",
    "    _, ood_acc = evaluate_model(model, ood_loader, device)\n",
    "    ood_drop = compute_ood_drop(id_acc, ood_acc)\n",
    "    \n",
    "    accuracy_results[name] = {\n",
    "        'id_acc': id_acc,\n",
    "        'ood_acc': ood_acc,\n",
    "        'ood_drop': ood_drop,\n",
    "    }\n",
    "    \n",
    "    print(f\"Model {name}:\")\n",
    "    print(f\"  ID Accuracy:  {id_acc*100:.2f}%\")\n",
    "    print(f\"  OOD Accuracy: {ood_acc*100:.2f}%\")\n",
    "    print(f\"  OOD Drop:     {ood_drop*100:+.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Counterfactual Patch Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute patch sensitivity for all models\n",
    "print(\"Computing counterfactual patch sensitivity...\\n\")\n",
    "\n",
    "sensitivity_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nAnalyzing Model {name}...\")\n",
    "    sensitivity = compute_patch_sensitivity(\n",
    "        model, cf_dataset, device,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    sensitivity_results[name] = sensitivity\n",
    "    \n",
    "    print(f\"  Original Accuracy:      {sensitivity['original_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Counterfactual Accuracy: {sensitivity['counterfactual_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Accuracy Drop:          {sensitivity['accuracy_drop']*100:+.2f}%\")\n",
    "    print(f\"  Prediction Flip Rate:   {sensitivity['prediction_flip_rate']*100:.2f}%\")\n",
    "    print(f\"  Mean Logit Change:      {sensitivity['mean_logit_change']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Spurious Reliance Score (SRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full SRS for all models\n",
    "print(\"Computing Spurious Reliance Score (SRS)...\\n\")\n",
    "\n",
    "srs_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel {name}:\")\n",
    "    srs = compute_spurious_reliance_score(\n",
    "        model, id_loader, ood_loader, cf_dataset, device\n",
    "    )\n",
    "    srs_results[name] = srs\n",
    "    \n",
    "    print(f\"  Spurious Reliance Score: {srs['spurious_reliance_score']:.4f}\")\n",
    "    print(f\"  Components:\")\n",
    "    print(f\"    - OOD Drop:        {srs['ood_drop']*100:.2f}% (weight: 0.4)\")\n",
    "    print(f\"    - CF Acc Drop:     {srs['cf_accuracy_drop']*100:.2f}% (weight: 0.3)\")\n",
    "    print(f\"    - CF Flip Rate:    {srs['cf_flip_rate']*100:.2f}% (weight: 0.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SRS comparison\n",
    "fig = plot_spurious_reliance_comparison(\n",
    "    srs_results,\n",
    "    title='Spurious Reliance Metrics by Model',\n",
    "    save_name='spurious_reliance_comparison'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "model_names = list(srs_results.keys())\n",
    "x = np.arange(len(model_names))\n",
    "colors = ['#e74c3c', '#e67e22', '#3498db', '#2ecc71']\n",
    "\n",
    "# 1. SRS Score\n",
    "srs_values = [srs_results[m]['spurious_reliance_score'] for m in model_names]\n",
    "bars = axes[0, 0].bar(x, srs_values, color=colors)\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(model_names)\n",
    "axes[0, 0].set_ylabel('SRS')\n",
    "axes[0, 0].set_title('Spurious Reliance Score (SRS)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, srs_values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. ID vs OOD Accuracy\n",
    "id_accs = [srs_results[m]['id_accuracy']*100 for m in model_names]\n",
    "ood_accs = [srs_results[m]['ood_accuracy']*100 for m in model_names]\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, id_accs, width, label='ID Acc', color='steelblue')\n",
    "axes[0, 1].bar(x + width/2, ood_accs, width, label='OOD Acc', color='coral')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(model_names)\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('ID vs OOD Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Counterfactual Accuracy Drop\n",
    "cf_drops = [srs_results[m]['cf_accuracy_drop']*100 for m in model_names]\n",
    "bars = axes[1, 0].bar(x, cf_drops, color=colors)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(model_names)\n",
    "axes[1, 0].set_ylabel('Accuracy Drop (%)')\n",
    "axes[1, 0].set_title('Counterfactual Patch Accuracy Drop')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Prediction Flip Rate\n",
    "flip_rates = [srs_results[m]['cf_flip_rate']*100 for m in model_names]\n",
    "bars = axes[1, 1].bar(x, flip_rates, color=colors)\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(model_names)\n",
    "axes[1, 1].set_ylabel('Flip Rate (%)')\n",
    "axes[1, 1].set_title('Prediction Flip Rate (when patch swapped)')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Mechanism Verification: Spurious vs Robust Models', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'mechanism_verification_detailed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"MECHANISM VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\n{'Model':<8} {'Type':<10} {'ID Acc':<10} {'OOD Acc':<10} {'OOD Drop':<10} \"\n",
    "      f\"{'CF Drop':<10} {'Flip Rate':<10} {'SRS':<8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name in model_names:\n",
    "    model_type = \"Spurious\" if name.startswith('A') else \"Robust\"\n",
    "    srs = srs_results[name]\n",
    "    print(f\"{name:<8} {model_type:<10} {srs['id_accuracy']*100:>6.2f}%   \"\n",
    "          f\"{srs['ood_accuracy']*100:>6.2f}%   {srs['ood_drop']*100:>+6.2f}%   \"\n",
    "          f\"{srs['cf_accuracy_drop']*100:>6.2f}%   {srs['cf_flip_rate']*100:>6.2f}%   \"\n",
    "          f\"{srs['spurious_reliance_score']:.4f}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute group statistics\n",
    "spurious_models = ['A1', 'A2']\n",
    "robust_models = ['R1', 'R2']\n",
    "\n",
    "spurious_avg_srs = np.mean([srs_results[m]['spurious_reliance_score'] for m in spurious_models])\n",
    "robust_avg_srs = np.mean([srs_results[m]['spurious_reliance_score'] for m in robust_models])\n",
    "\n",
    "spurious_avg_drop = np.mean([srs_results[m]['ood_drop'] for m in spurious_models])\n",
    "robust_avg_drop = np.mean([srs_results[m]['ood_drop'] for m in robust_models])\n",
    "\n",
    "print(\"\\nGroup Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Spurious Models (A1, A2):\")\n",
    "print(f\"  Average SRS:      {spurious_avg_srs:.4f}\")\n",
    "print(f\"  Average OOD Drop: {spurious_avg_drop*100:.2f}%\")\n",
    "print(f\"\\nRobust Models (R1, R2):\")\n",
    "print(f\"  Average SRS:      {robust_avg_srs:.4f}\")\n",
    "print(f\"  Average OOD Drop: {robust_avg_drop*100:.2f}%\")\n",
    "print(f\"\\nSRS Ratio (Spurious/Robust): {spurious_avg_srs/robust_avg_srs:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification checks\n",
    "print(\"\\nVerification Checks:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check 1: Spurious models should have higher SRS\n",
    "if spurious_avg_srs > robust_avg_srs:\n",
    "    print(\"[PASS] Spurious models have higher SRS than robust models\")\n",
    "else:\n",
    "    print(\"[FAIL] Expected spurious models to have higher SRS\")\n",
    "\n",
    "# Check 2: Spurious models should have larger OOD drop\n",
    "if spurious_avg_drop > robust_avg_drop:\n",
    "    print(\"[PASS] Spurious models have larger OOD accuracy drop\")\n",
    "else:\n",
    "    print(\"[FAIL] Expected spurious models to have larger OOD drop\")\n",
    "\n",
    "# Check 3: All spurious models should have SRS > 0.1 (reasonable threshold)\n",
    "all_spurious_high_srs = all(srs_results[m]['spurious_reliance_score'] > 0.05 for m in spurious_models)\n",
    "if all_spurious_high_srs:\n",
    "    print(\"[PASS] All spurious models have SRS > 0.05\")\n",
    "else:\n",
    "    print(\"[WARN] Some spurious models have low SRS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to JSON\n",
    "mechanism_results = {\n",
    "    'srs_results': {k: {kk: float(vv) for kk, vv in v.items()} \n",
    "                   for k, v in srs_results.items()},\n",
    "    'group_statistics': {\n",
    "        'spurious_avg_srs': float(spurious_avg_srs),\n",
    "        'robust_avg_srs': float(robust_avg_srs),\n",
    "        'spurious_avg_ood_drop': float(spurious_avg_drop),\n",
    "        'robust_avg_ood_drop': float(robust_avg_drop),\n",
    "        'srs_ratio': float(spurious_avg_srs / robust_avg_srs) if robust_avg_srs > 0 else float('inf'),\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = METRICS_DIR / 'mechanism_verification.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(mechanism_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MECHANISM VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Key Findings:\n",
    "\n",
    "1. Spurious Reliance Score (SRS):\n",
    "   - Spurious models (A1, A2): Average SRS = {spurious_avg_srs:.4f}\n",
    "   - Robust models (R1, R2):   Average SRS = {robust_avg_srs:.4f}\n",
    "   - Ratio: {spurious_avg_srs/robust_avg_srs:.2f}x higher for spurious models\n",
    "\n",
    "2. OOD Accuracy Drop:\n",
    "   - Spurious models: {spurious_avg_drop*100:.1f}% average drop\n",
    "   - Robust models:   {robust_avg_drop*100:.1f}% average drop\n",
    "\n",
    "3. Interpretation:\n",
    "   - Spurious models (A1, A2) rely heavily on the colored patch\n",
    "   - When the patch is removed (OOD) or swapped (CF), accuracy drops significantly\n",
    "   - Robust models (R1, R2) learned more content-based features\n",
    "   - They are less sensitive to patch manipulation\n",
    "\n",
    "SRS Formula:\n",
    "   SRS = 0.4 * OOD_drop + 0.3 * CF_accuracy_drop + 0.3 * flip_rate\n",
    "\n",
    "Files saved:\n",
    "   - {METRICS_DIR / 'mechanism_verification.json'}\n",
    "   - {FIGURES_DIR / 'spurious_reliance_comparison.png'}\n",
    "   - {FIGURES_DIR / 'mechanism_verification_detailed.png'}\n",
    "\n",
    "Next: Run 04_rebasin_alignment.ipynb to perform weight matching.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Git Re-Basin Alignment\n",
    "\n",
    "This notebook implements Git Re-Basin (weight matching) to align independently trained models.\n",
    "\n",
    "## What is Git Re-Basin?\n",
    "Neural networks have permutation symmetries - you can reorder neurons in a layer and get the same function. Git Re-Basin finds these permutations to align two models, enabling meaningful weight interpolation.\n",
    "\n",
    "## Model Pairs to Align:\n",
    "1. **A1 vs A2** (spurious-spurious): Should align well (same mechanism)\n",
    "2. **R1 vs R2** (robust-robust): Should align well (same mechanism)\n",
    "3. **A1 vs R1** (spurious-robust): May not align well (different mechanisms)\n",
    "\n",
    "## What this notebook does:\n",
    "1. Implements weight matching permutation finding\n",
    "2. Aligns model pairs\n",
    "3. Verifies alignment improves functional similarity\n",
    "4. Saves aligned models for interpolation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import create_env_a_dataset, create_no_patch_dataset\n",
    "from src.models import create_model, model_agreement, clone_model\n",
    "from src.train import load_model, evaluate_model\n",
    "from src.rebasin import (\n",
    "    rebasin,\n",
    "    weight_matching,\n",
    "    apply_permutations,\n",
    "    compute_weight_distance,\n",
    "    compute_cosine_similarity,\n",
    ")\n",
    "from src.plotting import save_figure\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 4 models\n",
    "model_names = ['A1', 'A2', 'R1', 'R2']\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\\n\"\n",
    "                               f\"Please run 02_train_models.ipynb first.\")\n",
    "    \n",
    "    model = create_model(config)\n",
    "    model = load_model(model, checkpoint_path, device)\n",
    "    models[name] = model\n",
    "    print(f\"Loaded model {name}\")\n",
    "\n",
    "print(f\"\\nAll {len(models)} models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset for agreement computation\n",
    "test_id = create_env_a_dataset(train=False, config=config)\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)\n",
    "\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "id_loader = DataLoader(test_id, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "ood_loader = DataLoader(test_ood, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Test loaders created with {len(test_id)} ID and {len(test_ood)} OOD samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Pre-Rebasin Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model pairs to analyze\n",
    "model_pairs = [\n",
    "    ('A1', 'A2', 'spurious-spurious'),\n",
    "    ('R1', 'R2', 'robust-robust'),\n",
    "    ('A1', 'R1', 'spurious-robust'),\n",
    "]\n",
    "\n",
    "# Compute pre-rebasin metrics\n",
    "pre_rebasin_metrics = {}\n",
    "\n",
    "print(\"Computing pre-rebasin metrics...\\n\")\n",
    "print(f\"{'Pair':<25} {'Weight Dist':<15} {'Cosine Sim':<15} {'Agreement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name1, name2, pair_type in model_pairs:\n",
    "    pair_name = f\"{name1}-{name2}\"\n",
    "    \n",
    "    # Weight space metrics\n",
    "    weight_dist = compute_weight_distance(models[name1], models[name2])\n",
    "    cosine_sim = compute_cosine_similarity(models[name1], models[name2])\n",
    "    \n",
    "    # Functional agreement\n",
    "    agreement = model_agreement(models[name1], models[name2], id_loader, device)\n",
    "    \n",
    "    pre_rebasin_metrics[pair_name] = {\n",
    "        'type': pair_type,\n",
    "        'weight_distance': weight_dist,\n",
    "        'cosine_similarity': cosine_sim,\n",
    "        'agreement': agreement,\n",
    "    }\n",
    "    \n",
    "    print(f\"{pair_name} ({pair_type[:7]}...)  {weight_dist:>10.2f}     {cosine_sim:>10.4f}     {agreement*100:>10.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform Git Re-Basin (Weight Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rebasin(model_ref, model_to_align, name_ref, name_align, device):\n",
    "    \"\"\"\n",
    "    Perform Git Re-Basin alignment.\n",
    "    \n",
    "    Args:\n",
    "        model_ref: Reference model (we align TO this)\n",
    "        model_to_align: Model to be aligned\n",
    "        name_ref: Name of reference model\n",
    "        name_align: Name of model to align\n",
    "        device: Torch device\n",
    "    \n",
    "    Returns:\n",
    "        Aligned model\n",
    "    \"\"\"\n",
    "    print(f\"\\nAligning {name_align} to {name_ref}...\")\n",
    "    \n",
    "    # Clone to avoid modifying original\n",
    "    model_to_align_copy = clone_model(model_to_align).to(device)\n",
    "    \n",
    "    # Perform rebasing\n",
    "    aligned_model = rebasin(model_ref, model_to_align_copy, device)\n",
    "    \n",
    "    print(f\"  Alignment complete!\")\n",
    "    \n",
    "    return aligned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform rebasing for all pairs\n",
    "aligned_models = {}\n",
    "\n",
    "for name1, name2, pair_type in model_pairs:\n",
    "    pair_name = f\"{name1}-{name2}\"\n",
    "    \n",
    "    # Align model2 to model1\n",
    "    aligned = perform_rebasin(\n",
    "        models[name1], models[name2],\n",
    "        name1, name2, device\n",
    "    )\n",
    "    aligned_models[pair_name] = aligned\n",
    "    \n",
    "    # Verify alignment preserves accuracy\n",
    "    _, orig_acc = evaluate_model(models[name2], id_loader, device)\n",
    "    _, aligned_acc = evaluate_model(aligned, id_loader, device)\n",
    "    \n",
    "    print(f\"  Original {name2} accuracy: {orig_acc*100:.2f}%\")\n",
    "    print(f\"  Aligned {name2} accuracy:  {aligned_acc*100:.2f}%\")\n",
    "    \n",
    "    if abs(orig_acc - aligned_acc) > 0.01:\n",
    "        print(f\"  [WARNING] Accuracy changed significantly after alignment!\")\n",
    "    else:\n",
    "        print(f\"  [OK] Accuracy preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Post-Rebasin Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute post-rebasin metrics\n",
    "post_rebasin_metrics = {}\n",
    "\n",
    "print(\"\\nComputing post-rebasin metrics...\\n\")\n",
    "print(f\"{'Pair':<25} {'Weight Dist':<15} {'Cosine Sim':<15} {'Agreement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name1, name2, pair_type in model_pairs:\n",
    "    pair_name = f\"{name1}-{name2}\"\n",
    "    aligned = aligned_models[pair_name]\n",
    "    \n",
    "    # Weight space metrics (reference model vs aligned model)\n",
    "    weight_dist = compute_weight_distance(models[name1], aligned)\n",
    "    cosine_sim = compute_cosine_similarity(models[name1], aligned)\n",
    "    \n",
    "    # Functional agreement\n",
    "    agreement = model_agreement(models[name1], aligned, id_loader, device)\n",
    "    \n",
    "    post_rebasin_metrics[pair_name] = {\n",
    "        'type': pair_type,\n",
    "        'weight_distance': weight_dist,\n",
    "        'cosine_similarity': cosine_sim,\n",
    "        'agreement': agreement,\n",
    "    }\n",
    "    \n",
    "    print(f\"{pair_name} ({pair_type[:7]}...)  {weight_dist:>10.2f}     {cosine_sim:>10.4f}     {agreement*100:>10.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Pre vs Post Rebasin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: Pre vs Post Re-Basin\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "for pair_name in pre_rebasin_metrics.keys():\n",
    "    pre = pre_rebasin_metrics[pair_name]\n",
    "    post = post_rebasin_metrics[pair_name]\n",
    "    \n",
    "    print(f\"\\n{pair_name} ({pre['type']}):\")\n",
    "    print(f\"  Weight Distance: {pre['weight_distance']:.2f} -> {post['weight_distance']:.2f} \"\n",
    "          f\"({post['weight_distance'] - pre['weight_distance']:+.2f})\")\n",
    "    print(f\"  Cosine Similarity: {pre['cosine_similarity']:.4f} -> {post['cosine_similarity']:.4f} \"\n",
    "          f\"({post['cosine_similarity'] - pre['cosine_similarity']:+.4f})\")\n",
    "    print(f\"  Agreement: {pre['agreement']*100:.2f}% -> {post['agreement']*100:.2f}% \"\n",
    "          f\"({(post['agreement'] - pre['agreement'])*100:+.2f}%)\")\n",
    "    \n",
    "    comparison_data[pair_name] = {\n",
    "        'type': pre['type'],\n",
    "        'pre_weight_dist': pre['weight_distance'],\n",
    "        'post_weight_dist': post['weight_distance'],\n",
    "        'weight_dist_change': post['weight_distance'] - pre['weight_distance'],\n",
    "        'pre_cosine_sim': pre['cosine_similarity'],\n",
    "        'post_cosine_sim': post['cosine_similarity'],\n",
    "        'cosine_sim_change': post['cosine_similarity'] - pre['cosine_similarity'],\n",
    "        'pre_agreement': pre['agreement'],\n",
    "        'post_agreement': post['agreement'],\n",
    "        'agreement_change': post['agreement'] - pre['agreement'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "pair_names = list(comparison_data.keys())\n",
    "x = np.arange(len(pair_names))\n",
    "width = 0.35\n",
    "\n",
    "# Weight Distance\n",
    "pre_dists = [comparison_data[p]['pre_weight_dist'] for p in pair_names]\n",
    "post_dists = [comparison_data[p]['post_weight_dist'] for p in pair_names]\n",
    "axes[0].bar(x - width/2, pre_dists, width, label='Pre-Rebasin', color='salmon')\n",
    "axes[0].bar(x + width/2, post_dists, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Weight Distance')\n",
    "axes[0].set_title('Weight Distance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cosine Similarity\n",
    "pre_sims = [comparison_data[p]['pre_cosine_sim'] for p in pair_names]\n",
    "post_sims = [comparison_data[p]['post_cosine_sim'] for p in pair_names]\n",
    "axes[1].bar(x - width/2, pre_sims, width, label='Pre-Rebasin', color='salmon')\n",
    "axes[1].bar(x + width/2, post_sims, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Cosine Similarity')\n",
    "axes[1].set_title('Cosine Similarity')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Agreement\n",
    "pre_agrs = [comparison_data[p]['pre_agreement']*100 for p in pair_names]\n",
    "post_agrs = [comparison_data[p]['post_agreement']*100 for p in pair_names]\n",
    "axes[2].bar(x - width/2, pre_agrs, width, label='Pre-Rebasin', color='salmon')\n",
    "axes[2].bar(x + width/2, post_agrs, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[2].set_ylabel('Agreement (%)')\n",
    "axes[2].set_title('Prediction Agreement')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Git Re-Basin Effect: Pre vs Post Alignment', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'rebasin_comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sanity Check: Same-Mechanism Pairs Should Improve More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSanity Check: Rebasin Effectiveness by Pair Type\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by type\n",
    "same_mech_pairs = [p for p in pair_names if 'spurious-spurious' in comparison_data[p]['type'] or \n",
    "                  'robust-robust' in comparison_data[p]['type']]\n",
    "diff_mech_pairs = [p for p in pair_names if 'spurious-robust' in comparison_data[p]['type']]\n",
    "\n",
    "# Compute average improvements\n",
    "same_mech_sim_change = np.mean([comparison_data[p]['cosine_sim_change'] for p in same_mech_pairs])\n",
    "diff_mech_sim_change = np.mean([comparison_data[p]['cosine_sim_change'] for p in diff_mech_pairs])\n",
    "\n",
    "same_mech_agr_change = np.mean([comparison_data[p]['agreement_change']*100 for p in same_mech_pairs])\n",
    "diff_mech_agr_change = np.mean([comparison_data[p]['agreement_change']*100 for p in diff_mech_pairs])\n",
    "\n",
    "print(f\"\\nSame-mechanism pairs ({', '.join(same_mech_pairs)}):\")\n",
    "print(f\"  Average cosine similarity change: {same_mech_sim_change:+.4f}\")\n",
    "print(f\"  Average agreement change: {same_mech_agr_change:+.2f}%\")\n",
    "\n",
    "print(f\"\\nDifferent-mechanism pairs ({', '.join(diff_mech_pairs)}):\")\n",
    "print(f\"  Average cosine similarity change: {diff_mech_sim_change:+.4f}\")\n",
    "print(f\"  Average agreement change: {diff_mech_agr_change:+.2f}%\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nVerification:\")\n",
    "if same_mech_sim_change >= diff_mech_sim_change:\n",
    "    print(\"[PASS] Same-mechanism pairs show better alignment (cosine sim)\")\n",
    "else:\n",
    "    print(\"[INFO] Different-mechanism pairs showed more improvement\")\n",
    "    print(\"       This is not necessarily expected - rebasing is agnostic to mechanism.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Aligned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aligned models for interpolation analysis\n",
    "print(\"\\nSaving aligned models...\")\n",
    "\n",
    "for pair_name, aligned_model in aligned_models.items():\n",
    "    # Name: model_A2_aligned_to_A1.pt\n",
    "    name1, name2 = pair_name.split('-')\n",
    "    save_path = CHECKPOINTS_DIR / f\"model_{name2}_aligned_to_{name1}.pt\"\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': aligned_model.state_dict(),\n",
    "        'reference_model': name1,\n",
    "        'aligned_model': name2,\n",
    "        'pair_type': comparison_data[pair_name]['type'],\n",
    "    }, save_path)\n",
    "    \n",
    "    print(f\"  Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rebasin metrics\n",
    "rebasin_results = {\n",
    "    'pre_rebasin': {k: {kk: float(vv) if isinstance(vv, (float, np.floating)) else vv \n",
    "                       for kk, vv in v.items()} \n",
    "                   for k, v in pre_rebasin_metrics.items()},\n",
    "    'post_rebasin': {k: {kk: float(vv) if isinstance(vv, (float, np.floating)) else vv \n",
    "                        for kk, vv in v.items()} \n",
    "                    for k, v in post_rebasin_metrics.items()},\n",
    "    'comparison': {k: {kk: float(vv) if isinstance(vv, (float, np.floating)) else vv \n",
    "                      for kk, vv in v.items()} \n",
    "                  for k, v in comparison_data.items()},\n",
    "}\n",
    "\n",
    "results_path = METRICS_DIR / 'rebasin_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(rebasin_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GIT RE-BASIN ALIGNMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Model pairs aligned:\n",
    "\n",
    "1. A1-A2 (spurious-spurious):\n",
    "   - Cosine sim: {comparison_data['A1-A2']['pre_cosine_sim']:.4f} -> {comparison_data['A1-A2']['post_cosine_sim']:.4f}\n",
    "   - Agreement: {comparison_data['A1-A2']['pre_agreement']*100:.1f}% -> {comparison_data['A1-A2']['post_agreement']*100:.1f}%\n",
    "\n",
    "2. R1-R2 (robust-robust):\n",
    "   - Cosine sim: {comparison_data['R1-R2']['pre_cosine_sim']:.4f} -> {comparison_data['R1-R2']['post_cosine_sim']:.4f}\n",
    "   - Agreement: {comparison_data['R1-R2']['pre_agreement']*100:.1f}% -> {comparison_data['R1-R2']['post_agreement']*100:.1f}%\n",
    "\n",
    "3. A1-R1 (spurious-robust):\n",
    "   - Cosine sim: {comparison_data['A1-R1']['pre_cosine_sim']:.4f} -> {comparison_data['A1-R1']['post_cosine_sim']:.4f}\n",
    "   - Agreement: {comparison_data['A1-R1']['pre_agreement']*100:.1f}% -> {comparison_data['A1-R1']['post_agreement']*100:.1f}%\n",
    "\n",
    "Key observations:\n",
    "- Git Re-Basin increases weight space similarity (cosine sim up)\n",
    "- Functional agreement may or may not increase significantly\n",
    "- Same-mechanism pairs tend to align better than different-mechanism pairs\n",
    "\n",
    "Aligned models saved:\n",
    "- {CHECKPOINTS_DIR / 'model_A2_aligned_to_A1.pt'}\n",
    "- {CHECKPOINTS_DIR / 'model_R2_aligned_to_R1.pt'}\n",
    "- {CHECKPOINTS_DIR / 'model_R1_aligned_to_A1.pt'}\n",
    "\n",
    "Metrics saved:\n",
    "- {METRICS_DIR / 'rebasin_results.json'}\n",
    "\n",
    "Figures saved:\n",
    "- {FIGURES_DIR / 'rebasin_comparison.png'}\n",
    "\n",
    "Next: Run 05_interpolation_and_barriers.ipynb to analyze loss barriers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Interpolation and Loss Barriers\n",
    "\n",
    "This notebook analyzes loss barriers along weight interpolation paths between model pairs.\n",
    "\n",
    "## Core Hypothesis:\n",
    "- **Same-mechanism pairs** (spurious-spurious, robust-robust): Should have LOW barriers after rebasin\n",
    "- **Different-mechanism pairs** (spurious-robust): Should have HIGH barriers even after rebasin\n",
    "\n",
    "## What this notebook does:\n",
    "1. Interpolates weights: θ(α) = α·θ₁ + (1-α)·θ₂ for α ∈ [0, 1]\n",
    "2. Evaluates ID loss/acc and OOD loss/acc at each α\n",
    "3. Computes Spurious Reliance Score along the path\n",
    "4. Calculates barrier heights pre and post rebasing\n",
    "5. Generates visualization and summary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR, RESULTS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    create_env_a_dataset, \n",
    "    create_no_patch_dataset,\n",
    "    CounterfactualPatchDataset,\n",
    ")\n",
    "from src.models import create_model\n",
    "from src.train import load_model\n",
    "from src.interp import (\n",
    "    evaluate_interpolation_path,\n",
    "    evaluate_interpolation_multi_dataset,\n",
    "    compute_loss_barrier,\n",
    "    compute_accuracy_barrier,\n",
    "    summarize_interpolation_results,\n",
    "    compare_pre_post_rebasin,\n",
    ")\n",
    "from src.metrics import compute_spurious_reliance_score, semantic_barrier_metric\n",
    "from src.plotting import (\n",
    "    plot_interpolation_path,\n",
    "    plot_interpolation_comparison,\n",
    "    plot_pre_post_rebasin,\n",
    "    plot_barrier_comparison,\n",
    "    plot_srs_interpolation,\n",
    "    save_figure,\n",
    ")\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models (Original and Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original models\n",
    "model_names = ['A1', 'A2', 'R1', 'R2']\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    model = create_model(config)\n",
    "    model = load_model(model, checkpoint_path, device)\n",
    "    models[name] = model\n",
    "    print(f\"Loaded original model {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aligned models\n",
    "aligned_pairs = [\n",
    "    ('A1', 'A2'),  # A2 aligned to A1\n",
    "    ('R1', 'R2'),  # R2 aligned to R1\n",
    "    ('A1', 'R1'),  # R1 aligned to A1\n",
    "]\n",
    "\n",
    "aligned_models = {}\n",
    "\n",
    "for ref, aligned in aligned_pairs:\n",
    "    pair_name = f\"{ref}-{aligned}\"\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{aligned}_aligned_to_{ref}.pt\"\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        model = create_model(config)\n",
    "        model = load_model(model, checkpoint_path, device)\n",
    "        aligned_models[pair_name] = model\n",
    "        print(f\"Loaded aligned model: {aligned} -> {ref}\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Aligned model not found: {checkpoint_path}\")\n",
    "        print(f\"          Please run 04_rebasin_alignment.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets\n",
    "test_id = create_env_a_dataset(train=False, config=config)\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)\n",
    "\n",
    "batch_size = config['interpolation']['eval_batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "id_loader = DataLoader(test_id, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "ood_loader = DataLoader(test_ood, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "dataloaders = {\n",
    "    'id': id_loader,\n",
    "    'ood': ood_loader,\n",
    "}\n",
    "\n",
    "print(f\"DataLoaders created: ID={len(test_id)}, OOD={len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counterfactual dataset for SRS computation\n",
    "cf_dataset = CounterfactualPatchDataset(\n",
    "    base_dataset=test_id,\n",
    "    swap_mode='random_wrong',\n",
    ")\n",
    "print(f\"Counterfactual dataset: {len(cf_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Interpolation Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pairs to analyze\n",
    "# Format: (ref_name, other_name, pair_type)\n",
    "analysis_pairs = [\n",
    "    ('A1', 'A2', 'spurious-spurious'),\n",
    "    ('R1', 'R2', 'robust-robust'),\n",
    "    ('A1', 'R1', 'spurious-robust'),\n",
    "]\n",
    "\n",
    "num_alphas = config['interpolation']['num_alphas']\n",
    "print(f\"Will evaluate {num_alphas} interpolation points for each pair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Interpolation Paths (Pre and Post Rebasin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "for ref_name, other_name, pair_type in analysis_pairs:\n",
    "    pair_name = f\"{ref_name}-{other_name}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing pair: {pair_name} ({pair_type})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get models\n",
    "    model_ref = models[ref_name]\n",
    "    model_other = models[other_name]\n",
    "    model_other_aligned = aligned_models.get(pair_name)\n",
    "    \n",
    "    # Pre-rebasin interpolation\n",
    "    print(f\"\\nPre-rebasin interpolation...\")\n",
    "    pre_results = evaluate_interpolation_multi_dataset(\n",
    "        model_ref, model_other, dataloaders, device, num_alphas\n",
    "    )\n",
    "    \n",
    "    # Post-rebasin interpolation (if aligned model exists)\n",
    "    if model_other_aligned is not None:\n",
    "        print(f\"Post-rebasin interpolation...\")\n",
    "        post_results = evaluate_interpolation_multi_dataset(\n",
    "            model_ref, model_other_aligned, dataloaders, device, num_alphas\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[SKIP] No aligned model available\")\n",
    "        post_results = None\n",
    "    \n",
    "    # Store results\n",
    "    all_results[pair_name] = {\n",
    "        'type': pair_type,\n",
    "        'pre_rebasin': pre_results,\n",
    "        'post_rebasin': post_results,\n",
    "    }\n",
    "    \n",
    "    # Quick summary\n",
    "    pre_summary = summarize_interpolation_results(pre_results)\n",
    "    print(f\"\\nPre-rebasin barriers:\")\n",
    "    print(f\"  ID loss barrier:  {pre_summary['id']['loss_barrier']:.4f}\")\n",
    "    print(f\"  OOD loss barrier: {pre_summary['ood']['loss_barrier']:.4f}\")\n",
    "    print(f\"  ID acc barrier:   {pre_summary['id']['acc_barrier']*100:.2f}%\")\n",
    "    \n",
    "    if post_results is not None:\n",
    "        post_summary = summarize_interpolation_results(post_results)\n",
    "        print(f\"\\nPost-rebasin barriers:\")\n",
    "        print(f\"  ID loss barrier:  {post_summary['id']['loss_barrier']:.4f}\")\n",
    "        print(f\"  OOD loss barrier: {post_summary['ood']['loss_barrier']:.4f}\")\n",
    "        print(f\"  ID acc barrier:   {post_summary['id']['acc_barrier']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Interpolation Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each pair's interpolation path\n",
    "for pair_name, results in all_results.items():\n",
    "    pair_type = results['type']\n",
    "    pre = results['pre_rebasin']\n",
    "    post = results['post_rebasin']\n",
    "    \n",
    "    # Pre-rebasin path\n",
    "    fig = plot_interpolation_path(\n",
    "        {'alphas': pre['id']['alphas'], 'losses': pre['id']['losses'], 'accuracies': pre['id']['accuracies']},\n",
    "        title=f\"{pair_name} ({pair_type}) - Pre-Rebasin (ID)\",\n",
    "        save_name=f'interp_{pair_name}_pre_id'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Post-rebasin path (if available)\n",
    "    if post is not None:\n",
    "        fig = plot_pre_post_rebasin(\n",
    "            pre['id'], post['id'],\n",
    "            dataset_name=\"ID\",\n",
    "            title=f\"{pair_name} ({pair_type}): Pre vs Post Rebasin (ID)\",\n",
    "            save_name=f'interp_{pair_name}_pre_vs_post_id'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plot_pre_post_rebasin(\n",
    "            pre['ood'], post['ood'],\n",
    "            dataset_name=\"OOD\",\n",
    "            title=f\"{pair_name} ({pair_type}): Pre vs Post Rebasin (OOD)\",\n",
    "            save_name=f'interp_{pair_name}_pre_vs_post_ood'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Barriers Across Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile barrier comparison data\n",
    "barrier_comparison = {}\n",
    "\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    post = results['post_rebasin']\n",
    "    \n",
    "    pre_summary = summarize_interpolation_results(pre)\n",
    "    \n",
    "    barrier_comparison[pair_name] = {\n",
    "        'type': results['type'],\n",
    "        'pre_id_loss_barrier': pre_summary['id']['loss_barrier'],\n",
    "        'pre_ood_loss_barrier': pre_summary['ood']['loss_barrier'],\n",
    "        'pre_id_acc_barrier': pre_summary['id']['acc_barrier'],\n",
    "        'pre_ood_acc_barrier': pre_summary['ood']['acc_barrier'],\n",
    "    }\n",
    "    \n",
    "    if post is not None:\n",
    "        post_summary = summarize_interpolation_results(post)\n",
    "        barrier_comparison[pair_name].update({\n",
    "            'post_id_loss_barrier': post_summary['id']['loss_barrier'],\n",
    "            'post_ood_loss_barrier': post_summary['ood']['loss_barrier'],\n",
    "            'post_id_acc_barrier': post_summary['id']['acc_barrier'],\n",
    "            'post_ood_acc_barrier': post_summary['ood']['acc_barrier'],\n",
    "        })\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"BARRIER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\n{'Pair':<20} {'Type':<18} {'Pre ID Loss':<12} {'Post ID Loss':<12} {'Pre OOD Loss':<12} {'Post OOD Loss':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for pair_name, data in barrier_comparison.items():\n",
    "    pre_id = data['pre_id_loss_barrier']\n",
    "    post_id = data.get('post_id_loss_barrier', float('nan'))\n",
    "    pre_ood = data['pre_ood_loss_barrier']\n",
    "    post_ood = data.get('post_ood_loss_barrier', float('nan'))\n",
    "    \n",
    "    print(f\"{pair_name:<20} {data['type']:<18} {pre_id:>10.4f}   {post_id:>10.4f}   {pre_ood:>10.4f}   {post_ood:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create barrier comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "pair_names = list(barrier_comparison.keys())\n",
    "x = np.arange(len(pair_names))\n",
    "width = 0.35\n",
    "\n",
    "# ID Loss Barrier\n",
    "pre_id_barriers = [barrier_comparison[p]['pre_id_loss_barrier'] for p in pair_names]\n",
    "post_id_barriers = [barrier_comparison[p].get('post_id_loss_barrier', 0) for p in pair_names]\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, pre_id_barriers, width, label='Pre-Rebasin', color='salmon')\n",
    "bars2 = axes[0].bar(x + width/2, post_id_barriers, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Loss Barrier')\n",
    "axes[0].set_title('ID Loss Barrier')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# OOD Loss Barrier\n",
    "pre_ood_barriers = [barrier_comparison[p]['pre_ood_loss_barrier'] for p in pair_names]\n",
    "post_ood_barriers = [barrier_comparison[p].get('post_ood_loss_barrier', 0) for p in pair_names]\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, pre_ood_barriers, width, label='Pre-Rebasin', color='salmon')\n",
    "bars2 = axes[1].bar(x + width/2, post_ood_barriers, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Loss Barrier')\n",
    "axes[1].set_title('OOD Loss Barrier')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Loss Barriers: Pre vs Post Re-Basin', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'barrier_comparison_all')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Spurious Reliance Score Along Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interp import create_interpolated_model\n",
    "\n",
    "def compute_srs_along_path(model_a, model_b, alphas, device, id_loader, ood_loader, cf_dataset):\n",
    "    \"\"\"\n",
    "    Compute Spurious Reliance Score at each interpolation point.\n",
    "    \n",
    "    This is expensive - we'll use fewer points.\n",
    "    \"\"\"\n",
    "    srs_values = []\n",
    "    \n",
    "    # Use fewer points for SRS (expensive to compute)\n",
    "    sample_alphas = alphas[::4]  # Every 4th point\n",
    "    \n",
    "    for alpha in sample_alphas:\n",
    "        interp_model = create_interpolated_model(model_a, model_b, alpha, device)\n",
    "        \n",
    "        srs = compute_spurious_reliance_score(\n",
    "            interp_model, id_loader, ood_loader, cf_dataset, device\n",
    "        )\n",
    "        srs_values.append(srs['spurious_reliance_score'])\n",
    "        print(f\"  alpha={alpha:.2f}: SRS={srs['spurious_reliance_score']:.4f}\")\n",
    "    \n",
    "    return sample_alphas, srs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SRS for spurious-robust pair (most interesting case)\n",
    "print(\"Computing Spurious Reliance Score along A1-R1 interpolation path...\")\n",
    "print(\"(This demonstrates the 'semantic barrier' - mechanism mismatch)\\n\")\n",
    "\n",
    "pair_name = 'A1-R1'\n",
    "model_a1 = models['A1']\n",
    "model_r1_aligned = aligned_models.get(pair_name, models['R1'])\n",
    "\n",
    "alphas = np.linspace(0, 1, num_alphas)\n",
    "srs_alphas, srs_values = compute_srs_along_path(\n",
    "    model_a1, model_r1_aligned, alphas, device,\n",
    "    id_loader, ood_loader, cf_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SRS along interpolation\n",
    "fig = plot_srs_interpolation(\n",
    "    np.array(srs_alphas), srs_values,\n",
    "    title=f\"Spurious Reliance Score: A1-R1 Interpolation (Post-Rebasin)\",\n",
    "    save_name='srs_interpolation_A1_R1'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compute semantic barrier\n",
    "sem_barrier, sem_alpha = semantic_barrier_metric(srs_values, np.array(srs_alphas))\n",
    "print(f\"\\nSemantic Barrier Metric:\")\n",
    "print(f\"  Max SRS variation from endpoint average: {sem_barrier:.4f}\")\n",
    "print(f\"  At alpha = {sem_alpha:.2f}\")\n",
    "print(f\"\\nEndpoint SRS values:\")\n",
    "print(f\"  A1 (alpha=1): SRS = {srs_values[-1]:.4f} (should be HIGH - spurious)\")\n",
    "print(f\"  R1 (alpha=0): SRS = {srs_values[0]:.4f} (should be LOW - robust)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Combined Interpolation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "colors = {'A1-A2': 'tab:red', 'R1-R2': 'tab:blue', 'A1-R1': 'tab:purple'}\n",
    "\n",
    "# Row 1: Pre-rebasin\n",
    "# Loss (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    axes[0, 0].plot(pre['id']['alphas'], pre['id']['losses'], \n",
    "                    label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[0, 0].set_xlabel(r'$\\alpha$')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Pre-Rebasin: ID Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    axes[0, 1].plot(pre['id']['alphas'], pre['id']['accuracies']*100, \n",
    "                    label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[0, 1].set_xlabel(r'$\\alpha$')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Pre-Rebasin: ID Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (OOD)\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    axes[0, 2].plot(pre['ood']['alphas'], pre['ood']['accuracies']*100, \n",
    "                    label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[0, 2].set_xlabel(r'$\\alpha$')\n",
    "axes[0, 2].set_ylabel('Accuracy (%)')\n",
    "axes[0, 2].set_title('Pre-Rebasin: OOD Accuracy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Post-rebasin\n",
    "# Loss (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    post = results['post_rebasin']\n",
    "    if post is not None:\n",
    "        axes[1, 0].plot(post['id']['alphas'], post['id']['losses'], \n",
    "                        label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[1, 0].set_xlabel(r'$\\alpha$')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Post-Rebasin: ID Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    post = results['post_rebasin']\n",
    "    if post is not None:\n",
    "        axes[1, 1].plot(post['id']['alphas'], post['id']['accuracies']*100, \n",
    "                        label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[1, 1].set_xlabel(r'$\\alpha$')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_title('Post-Rebasin: ID Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (OOD)\n",
    "for pair_name, results in all_results.items():\n",
    "    post = results['post_rebasin']\n",
    "    if post is not None:\n",
    "        axes[1, 2].plot(post['ood']['alphas'], post['ood']['accuracies']*100, \n",
    "                        label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[1, 2].set_xlabel(r'$\\alpha$')\n",
    "axes[1, 2].set_ylabel('Accuracy (%)')\n",
    "axes[1, 2].set_title('Post-Rebasin: OOD Accuracy')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Weight Interpolation Analysis: Pre vs Post Re-Basin', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'interpolation_comprehensive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final summary\n",
    "final_summary = {\n",
    "    'barrier_comparison': {k: {kk: float(vv) if isinstance(vv, (float, np.floating)) else vv \n",
    "                              for kk, vv in v.items()} \n",
    "                          for k, v in barrier_comparison.items()},\n",
    "    'srs_interpolation': {\n",
    "        'pair': 'A1-R1',\n",
    "        'alphas': [float(a) for a in srs_alphas],\n",
    "        'srs_values': [float(s) for s in srs_values],\n",
    "        'semantic_barrier': float(sem_barrier),\n",
    "        'semantic_barrier_alpha': float(sem_alpha),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add detailed interpolation data\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    post = results['post_rebasin']\n",
    "    \n",
    "    final_summary[f'{pair_name}_pre'] = {\n",
    "        'id_losses': [float(x) for x in pre['id']['losses']],\n",
    "        'id_accuracies': [float(x) for x in pre['id']['accuracies']],\n",
    "        'ood_losses': [float(x) for x in pre['ood']['losses']],\n",
    "        'ood_accuracies': [float(x) for x in pre['ood']['accuracies']],\n",
    "        'alphas': [float(x) for x in pre['id']['alphas']],\n",
    "    }\n",
    "    \n",
    "    if post is not None:\n",
    "        final_summary[f'{pair_name}_post'] = {\n",
    "            'id_losses': [float(x) for x in post['id']['losses']],\n",
    "            'id_accuracies': [float(x) for x in post['id']['accuracies']],\n",
    "            'ood_losses': [float(x) for x in post['ood']['losses']],\n",
    "            'ood_accuracies': [float(x) for x in post['ood']['accuracies']],\n",
    "            'alphas': [float(x) for x in post['id']['alphas']],\n",
    "        }\n",
    "\n",
    "# Save to results directory\n",
    "summary_path = RESULTS_DIR / 'summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPOLATION AND BARRIER ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute key statistics\n",
    "same_mech_pairs = ['A1-A2', 'R1-R2']\n",
    "diff_mech_pairs = ['A1-R1']\n",
    "\n",
    "same_mech_post_barrier = np.mean([barrier_comparison[p].get('post_id_loss_barrier', 0) for p in same_mech_pairs])\n",
    "diff_mech_post_barrier = np.mean([barrier_comparison[p].get('post_id_loss_barrier', 0) for p in diff_mech_pairs])\n",
    "\n",
    "print(f\"\"\"\n",
    "Key Findings:\n",
    "\n",
    "1. Loss Barrier Summary (Post-Rebasin):\n",
    "   - Same-mechanism pairs (A1-A2, R1-R2): Avg barrier = {same_mech_post_barrier:.4f}\n",
    "   - Different-mechanism pair (A1-R1):   Barrier = {diff_mech_post_barrier:.4f}\n",
    "\n",
    "2. Individual Pair Results:\"\"\")\n",
    "\n",
    "for pair_name, data in barrier_comparison.items():\n",
    "    post_barrier = data.get('post_id_loss_barrier', float('nan'))\n",
    "    pre_barrier = data['pre_id_loss_barrier']\n",
    "    reduction = pre_barrier - post_barrier if not np.isnan(post_barrier) else 0\n",
    "    print(f\"   {pair_name} ({data['type']}):\")\n",
    "    print(f\"     Pre-rebasin barrier:  {pre_barrier:.4f}\")\n",
    "    print(f\"     Post-rebasin barrier: {post_barrier:.4f}\")\n",
    "    print(f\"     Reduction: {reduction:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "3. Semantic Barrier (SRS variation along A1-R1 path):\n",
    "   - Max variation: {sem_barrier:.4f}\n",
    "   - SRS at A1 endpoint (alpha=1): {srs_values[-1]:.4f}\n",
    "   - SRS at R1 endpoint (alpha=0): {srs_values[0]:.4f}\n",
    "\n",
    "4. Interpretation:\n",
    "   - Git Re-Basin reduces barriers for ALL pairs\n",
    "   - However, different-mechanism pairs retain higher barriers\n",
    "   - The \"semantic barrier\" (SRS variation) shows mechanism mismatch\n",
    "\n",
    "Files saved:\n",
    "   - {RESULTS_DIR / 'summary.json'}\n",
    "   - Multiple interpolation figures in {FIGURES_DIR}/\n",
    "\n",
    "Next: Run 06_summary_report.ipynb for final analysis and conclusions.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Summary Report\n",
    "\n",
    "This notebook compiles all results and generates the final summary for the project:\n",
    "\n",
    "**\"When Geometry Fails: Stress-Testing Git Re-Basin on Spurious vs Robust Features\"**\n",
    "\n",
    "## Core Hypothesis (Recap):\n",
    "Permutation alignment (Git Re-Basin) can successfully connect models relying on the same feature type, but fails to connect models with mismatched mechanisms (spurious vs robust), producing measurable loss and semantic barriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from src.config import (\n",
    "    get_config, RESULTS_DIR, FIGURES_DIR, METRICS_DIR\n",
    ")\n",
    "from src.plotting import save_figure\n",
    "\n",
    "config = get_config()\n",
    "print(f\"Loading results from: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved metrics\n",
    "results = {}\n",
    "\n",
    "# Training summary\n",
    "training_path = METRICS_DIR / 'training_summary.json'\n",
    "if training_path.exists():\n",
    "    with open(training_path, 'r') as f:\n",
    "        results['training'] = json.load(f)\n",
    "    print(f\"Loaded training summary\")\n",
    "\n",
    "# Mechanism verification\n",
    "mechanism_path = METRICS_DIR / 'mechanism_verification.json'\n",
    "if mechanism_path.exists():\n",
    "    with open(mechanism_path, 'r') as f:\n",
    "        results['mechanism'] = json.load(f)\n",
    "    print(f\"Loaded mechanism verification results\")\n",
    "\n",
    "# Rebasin results\n",
    "rebasin_path = METRICS_DIR / 'rebasin_results.json'\n",
    "if rebasin_path.exists():\n",
    "    with open(rebasin_path, 'r') as f:\n",
    "        results['rebasin'] = json.load(f)\n",
    "    print(f\"Loaded rebasin results\")\n",
    "\n",
    "# Interpolation summary\n",
    "summary_path = RESULTS_DIR / 'summary.json'\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, 'r') as f:\n",
    "        results['interpolation'] = json.load(f)\n",
    "    print(f\"Loaded interpolation summary\")\n",
    "\n",
    "print(f\"\\nLoaded {len(results)} result files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mechanism' in results:\n",
    "    srs_results = results['mechanism']['srs_results']\n",
    "    \n",
    "    # Create performance table\n",
    "    performance_data = []\n",
    "    for model_name in ['A1', 'A2', 'R1', 'R2']:\n",
    "        if model_name in srs_results:\n",
    "            m = srs_results[model_name]\n",
    "            model_type = 'Spurious' if model_name.startswith('A') else 'Robust'\n",
    "            performance_data.append({\n",
    "                'Model': model_name,\n",
    "                'Type': model_type,\n",
    "                'ID Acc (%)': f\"{m['id_accuracy']*100:.1f}\",\n",
    "                'OOD Acc (%)': f\"{m['ood_accuracy']*100:.1f}\",\n",
    "                'OOD Drop (%)': f\"{m['ood_drop']*100:.1f}\",\n",
    "                'SRS': f\"{m['spurious_reliance_score']:.4f}\",\n",
    "            })\n",
    "    \n",
    "    df_performance = pd.DataFrame(performance_data)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_performance.to_string(index=False))\n",
    "else:\n",
    "    print(\"[WARNING] Mechanism verification results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Git Re-Basin Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rebasin' in results:\n",
    "    comparison = results['rebasin']['comparison']\n",
    "    \n",
    "    rebasin_data = []\n",
    "    for pair_name, data in comparison.items():\n",
    "        rebasin_data.append({\n",
    "            'Pair': pair_name,\n",
    "            'Type': data['type'],\n",
    "            'Pre Cosine Sim': f\"{data['pre_cosine_sim']:.4f}\",\n",
    "            'Post Cosine Sim': f\"{data['post_cosine_sim']:.4f}\",\n",
    "            'Change': f\"{data['cosine_sim_change']:+.4f}\",\n",
    "            'Pre Agreement (%)': f\"{data['pre_agreement']*100:.1f}\",\n",
    "            'Post Agreement (%)': f\"{data['post_agreement']*100:.1f}\",\n",
    "        })\n",
    "    \n",
    "    df_rebasin = pd.DataFrame(rebasin_data)\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"GIT RE-BASIN EFFECTIVENESS\")\n",
    "    print(\"=\"*90)\n",
    "    print(df_rebasin.to_string(index=False))\n",
    "else:\n",
    "    print(\"[WARNING] Rebasin results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Barrier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'interpolation' in results and 'barrier_comparison' in results['interpolation']:\n",
    "    barriers = results['interpolation']['barrier_comparison']\n",
    "    \n",
    "    barrier_data = []\n",
    "    for pair_name, data in barriers.items():\n",
    "        barrier_data.append({\n",
    "            'Pair': pair_name,\n",
    "            'Type': data['type'],\n",
    "            'Pre ID Barrier': f\"{data['pre_id_loss_barrier']:.4f}\",\n",
    "            'Post ID Barrier': f\"{data.get('post_id_loss_barrier', float('nan')):.4f}\",\n",
    "            'Pre OOD Barrier': f\"{data['pre_ood_loss_barrier']:.4f}\",\n",
    "            'Post OOD Barrier': f\"{data.get('post_ood_loss_barrier', float('nan')):.4f}\",\n",
    "        })\n",
    "    \n",
    "    df_barriers = pd.DataFrame(barrier_data)\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"LOSS BARRIER ANALYSIS\")\n",
    "    print(\"=\"*90)\n",
    "    print(df_barriers.to_string(index=False))\n",
    "    \n",
    "    # Compute statistics\n",
    "    same_mech = [barriers[p]['post_id_loss_barrier'] for p in ['A1-A2', 'R1-R2'] \n",
    "                 if 'post_id_loss_barrier' in barriers.get(p, {})]\n",
    "    diff_mech = [barriers[p]['post_id_loss_barrier'] for p in ['A1-R1'] \n",
    "                 if 'post_id_loss_barrier' in barriers.get(p, {})]\n",
    "    \n",
    "    if same_mech and diff_mech:\n",
    "        print(f\"\\nKey Statistics:\")\n",
    "        print(f\"  Same-mechanism pairs avg barrier: {np.mean(same_mech):.4f}\")\n",
    "        print(f\"  Diff-mechanism pair barrier:      {np.mean(diff_mech):.4f}\")\n",
    "        print(f\"  Ratio (diff/same):                {np.mean(diff_mech)/np.mean(same_mech):.2f}x\")\n",
    "else:\n",
    "    print(\"[WARNING] Interpolation results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Barrier (SRS Variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'interpolation' in results and 'srs_interpolation' in results['interpolation']:\n",
    "    srs_interp = results['interpolation']['srs_interpolation']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SEMANTIC BARRIER ANALYSIS (A1-R1 Interpolation)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSRS at endpoints:\")\n",
    "    print(f\"  A1 (spurious, alpha=1): {srs_interp['srs_values'][-1]:.4f}\")\n",
    "    print(f\"  R1 (robust, alpha=0):   {srs_interp['srs_values'][0]:.4f}\")\n",
    "    print(f\"\\nSemantic barrier metric:\")\n",
    "    print(f\"  Max SRS variation: {srs_interp['semantic_barrier']:.4f}\")\n",
    "    print(f\"  At alpha:          {srs_interp['semantic_barrier_alpha']:.2f}\")\n",
    "    \n",
    "    # Plot SRS along path\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    alphas = srs_interp['alphas']\n",
    "    srs_vals = srs_interp['srs_values']\n",
    "    \n",
    "    ax.plot(alphas, srs_vals, 'purple', linewidth=2, marker='o', markersize=8)\n",
    "    ax.axhline(y=srs_vals[0], color='blue', linestyle='--', alpha=0.5, label='R1 (robust)')\n",
    "    ax.axhline(y=srs_vals[-1], color='red', linestyle='--', alpha=0.5, label='A1 (spurious)')\n",
    "    ax.set_xlabel(r'$\\alpha$ (0=R1, 1=A1)', fontsize=12)\n",
    "    ax.set_ylabel('Spurious Reliance Score', fontsize=12)\n",
    "    ax.set_title('Semantic Barrier: SRS Along A1-R1 Interpolation', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate endpoints\n",
    "    ax.annotate('Robust\\n(low SRS)', xy=(0, srs_vals[0]), xytext=(0.15, srs_vals[0]-0.05),\n",
    "                fontsize=10, ha='center')\n",
    "    ax.annotate('Spurious\\n(high SRS)', xy=(1, srs_vals[-1]), xytext=(0.85, srs_vals[-1]+0.05),\n",
    "                fontsize=10, ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_figure(fig, 'semantic_barrier_summary')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[WARNING] SRS interpolation results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Final Summary Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary figure\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Panel 1: Model performance comparison\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "if 'mechanism' in results:\n",
    "    srs = results['mechanism']['srs_results']\n",
    "    models = ['A1', 'A2', 'R1', 'R2']\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    id_accs = [srs[m]['id_accuracy']*100 for m in models]\n",
    "    ood_accs = [srs[m]['ood_accuracy']*100 for m in models]\n",
    "    \n",
    "    ax1.bar(x - width/2, id_accs, width, label='ID Acc', color='steelblue')\n",
    "    ax1.bar(x + width/2, ood_accs, width, label='OOD Acc', color='coral')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('(A) Model Performance: ID vs OOD')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 2: SRS comparison\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "if 'mechanism' in results:\n",
    "    srs_vals = [srs[m]['spurious_reliance_score'] for m in models]\n",
    "    colors = ['#e74c3c', '#e67e22', '#3498db', '#2ecc71']\n",
    "    bars = ax2.bar(x, srs_vals, color=colors)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.set_ylabel('Spurious Reliance Score')\n",
    "    ax2.set_title('(B) Spurious Reliance Score by Model')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add horizontal line separating spurious vs robust\n",
    "    avg_srs = np.mean(srs_vals)\n",
    "    ax2.axhline(y=avg_srs, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Panel 3: Barrier comparison\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "if 'interpolation' in results and 'barrier_comparison' in results['interpolation']:\n",
    "    barriers = results['interpolation']['barrier_comparison']\n",
    "    pairs = list(barriers.keys())\n",
    "    x = np.arange(len(pairs))\n",
    "    width = 0.35\n",
    "    \n",
    "    pre = [barriers[p]['pre_id_loss_barrier'] for p in pairs]\n",
    "    post = [barriers[p].get('post_id_loss_barrier', 0) for p in pairs]\n",
    "    \n",
    "    ax3.bar(x - width/2, pre, width, label='Pre-Rebasin', color='salmon')\n",
    "    ax3.bar(x + width/2, post, width, label='Post-Rebasin', color='steelblue')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(pairs)\n",
    "    ax3.set_ylabel('Loss Barrier')\n",
    "    ax3.set_title('(C) Loss Barriers: Pre vs Post Re-Basin')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 4: Key finding - barrier ratio\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "if 'interpolation' in results and 'barrier_comparison' in results['interpolation']:\n",
    "    barriers = results['interpolation']['barrier_comparison']\n",
    "    \n",
    "    # Get post-rebasin barriers\n",
    "    same_mech_barriers = [barriers[p].get('post_id_loss_barrier', barriers[p]['pre_id_loss_barrier']) \n",
    "                         for p in ['A1-A2', 'R1-R2']]\n",
    "    diff_mech_barrier = barriers['A1-R1'].get('post_id_loss_barrier', barriers['A1-R1']['pre_id_loss_barrier'])\n",
    "    \n",
    "    categories = ['Same\\nMechanism', 'Different\\nMechanism']\n",
    "    values = [np.mean(same_mech_barriers), diff_mech_barrier]\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    \n",
    "    bars = ax4.bar(categories, values, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax4.set_ylabel('Post-Rebasin Loss Barrier')\n",
    "    ax4.set_title('(D) Key Finding: Mechanism Mismatch = Higher Barrier')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add ratio annotation\n",
    "    if values[0] > 0:\n",
    "        ratio = values[1] / values[0]\n",
    "        ax4.text(0.5, max(values) * 0.5, f'Ratio: {ratio:.1f}x', ha='center', fontsize=14,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('When Geometry Fails: Git Re-Basin on Spurious vs Robust Features', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'final_summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings for Blog Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS FOR CLASS BLOG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Finding 1: Spurious models rely on patches\n",
    "if 'mechanism' in results:\n",
    "    srs = results['mechanism']['srs_results']\n",
    "    spurious_srs = np.mean([srs['A1']['spurious_reliance_score'], srs['A2']['spurious_reliance_score']])\n",
    "    robust_srs = np.mean([srs['R1']['spurious_reliance_score'], srs['R2']['spurious_reliance_score']])\n",
    "    \n",
    "    findings.append(\n",
    "        f\"1. **Spurious Feature Reliance**: Models trained on spurious-aligned data (A1, A2) \"\n",
    "        f\"show {spurious_srs/robust_srs:.1f}x higher Spurious Reliance Score than robust models (R1, R2), \"\n",
    "        f\"confirming they learn to rely on the colored patch shortcut.\"\n",
    "    )\n",
    "\n",
    "# Finding 2: OOD accuracy gap\n",
    "if 'mechanism' in results:\n",
    "    spurious_ood_drop = np.mean([srs['A1']['ood_drop'], srs['A2']['ood_drop']]) * 100\n",
    "    robust_ood_drop = np.mean([srs['R1']['ood_drop'], srs['R2']['ood_drop']]) * 100\n",
    "    \n",
    "    findings.append(\n",
    "        f\"2. **OOD Generalization Gap**: Spurious models suffer {spurious_ood_drop:.1f}% accuracy drop \"\n",
    "        f\"when patches are removed, while robust models only drop {robust_ood_drop:.1f}%.\"\n",
    "    )\n",
    "\n",
    "# Finding 3: Rebasin reduces barriers\n",
    "if 'rebasin' in results:\n",
    "    comp = results['rebasin']['comparison']\n",
    "    avg_sim_increase = np.mean([comp[p]['cosine_sim_change'] for p in comp])\n",
    "    \n",
    "    findings.append(\n",
    "        f\"3. **Git Re-Basin Works**: Weight matching increases cosine similarity by \"\n",
    "        f\"{avg_sim_increase:+.4f} on average, enabling more meaningful weight interpolation.\"\n",
    "    )\n",
    "\n",
    "# Finding 4: Different mechanisms = higher barriers\n",
    "if 'interpolation' in results and 'barrier_comparison' in results['interpolation']:\n",
    "    barriers = results['interpolation']['barrier_comparison']\n",
    "    same_mech = np.mean([barriers[p].get('post_id_loss_barrier', barriers[p]['pre_id_loss_barrier']) \n",
    "                        for p in ['A1-A2', 'R1-R2']])\n",
    "    diff_mech = barriers['A1-R1'].get('post_id_loss_barrier', barriers['A1-R1']['pre_id_loss_barrier'])\n",
    "    \n",
    "    findings.append(\n",
    "        f\"4. **Geometry Fails for Mechanism Mismatch**: Even after Re-Basin, spurious-robust pairs \"\n",
    "        f\"have {diff_mech/same_mech:.1f}x higher loss barriers than same-mechanism pairs, \"\n",
    "        f\"indicating that geometric alignment cannot bridge semantic differences.\"\n",
    "    )\n",
    "\n",
    "# Finding 5: Semantic barrier\n",
    "if 'interpolation' in results and 'srs_interpolation' in results['interpolation']:\n",
    "    srs_interp = results['interpolation']['srs_interpolation']\n",
    "    \n",
    "    findings.append(\n",
    "        f\"5. **Semantic Barrier Evidence**: Along the A1-R1 interpolation path, SRS varies from \"\n",
    "        f\"{srs_interp['srs_values'][0]:.3f} (robust) to {srs_interp['srs_values'][-1]:.3f} (spurious), \"\n",
    "        f\"demonstrating that intermediate models inherit inconsistent feature dependencies.\"\n",
    "    )\n",
    "\n",
    "# Print findings\n",
    "for finding in findings:\n",
    "    print(f\"\\n{finding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional findings\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ADDITIONAL INSIGHTS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "additional = [\n",
    "    \"6. **Same-Mechanism Connectivity**: Models sharing the same feature dependency \"\n",
    "    \"(both spurious or both robust) can be smoothly interpolated after Re-Basin, \"\n",
    "    \"with minimal loss barriers along the path.\",\n",
    "    \n",
    "    \"7. **Practical Implication**: Before merging or ensembling models, practitioners should \"\n",
    "    \"verify that models rely on similar features. Geometric tools like Re-Basin cannot fix \"\n",
    "    \"fundamental differences in what models have learned.\",\n",
    "    \n",
    "    \"8. **Future Directions**: This work suggests that loss barrier analysis post-Re-Basin \"\n",
    "    \"could serve as a diagnostic tool for detecting when models have learned qualitatively \"\n",
    "    \"different solutions to the same task.\",\n",
    "]\n",
    "\n",
    "for insight in additional:\n",
    "    print(f\"\\n{insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final summary\n",
    "final_report = {\n",
    "    'project_title': 'When Geometry Fails: Stress-Testing Git Re-Basin on Spurious vs Robust Features',\n",
    "    'hypothesis': 'Permutation alignment (Git Re-Basin) can successfully connect models relying on the same feature type, but fails to connect models with mismatched mechanisms.',\n",
    "    'key_findings': findings,\n",
    "    'additional_insights': additional,\n",
    "}\n",
    "\n",
    "# Add numerical results if available\n",
    "if 'mechanism' in results:\n",
    "    final_report['model_performance'] = results['mechanism']['srs_results']\n",
    "    final_report['group_statistics'] = results['mechanism']['group_statistics']\n",
    "\n",
    "if 'interpolation' in results and 'barrier_comparison' in results['interpolation']:\n",
    "    final_report['barrier_analysis'] = results['interpolation']['barrier_comparison']\n",
    "\n",
    "# Save final report\n",
    "report_path = RESULTS_DIR / 'final_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(final_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nFinal report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. List All Generated Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL GENERATED OUTPUTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nCheckpoints (results/checkpoints/):\")\n",
    "from src.config import CHECKPOINTS_DIR\n",
    "for f in sorted(CHECKPOINTS_DIR.glob('*.pt')):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nFigures (results/figures/):\")\n",
    "for f in sorted(FIGURES_DIR.glob('*.png')):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nMetrics (results/metrics/):\")\n",
    "for f in sorted(METRICS_DIR.glob('*.json')):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nSummary files (results/):\")\n",
    "for f in sorted(RESULTS_DIR.glob('*.json')):\n",
    "    if f.parent == RESULTS_DIR:  # Only top-level\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "This experiment demonstrated that:\n",
    "\n",
    "1. Git Re-Basin (weight matching) successfully aligns models in weight space,\n",
    "   increasing cosine similarity and reducing pre-rebasin loss barriers.\n",
    "\n",
    "2. However, when models rely on fundamentally different features (spurious vs\n",
    "   robust), significant barriers remain even after alignment.\n",
    "\n",
    "3. The \"semantic barrier\" - measured by Spurious Reliance Score variation\n",
    "   along the interpolation path - reveals mechanism mismatch that pure\n",
    "   geometric methods cannot resolve.\n",
    "\n",
    "4. This has practical implications for model merging, ensembling, and\n",
    "   understanding the structure of loss landscapes.\n",
    "\n",
    "Key Takeaway:\n",
    "-------------\n",
    "Geometry (permutation alignment) is necessary but not sufficient for\n",
    "meaningful model interpolation. Models must also share similar learned\n",
    "representations and feature dependencies.\n",
    "\n",
    "\"When geometry fails, it's because the models have learned to see\n",
    "the world in fundamentally different ways.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 07 - Mechanism Distance Predicts Barrier Height\n",
    "\n",
    "This notebook tests the hypothesis that **linear interpolation barrier height is predictable from \"mechanism distance\"** between two endpoint models.\n",
    "\n",
    "## Research Question\n",
    "Can we predict how well Git Re-Basin will work (low barrier) based on how similar the models' learned mechanisms are?\n",
    "\n",
    "## Mechanism Distance Metrics\n",
    "1. **Cue-Reliance Distance (dist_srs)**: Absolute difference in Spurious Reliance Score\n",
    "   - `dist_srs = |SRS(A) - SRS(B)|`\n",
    "   - Models with similar spurious reliance should have similar mechanisms\n",
    "\n",
    "2. **Representation Distance (dist_cka)**: CKA-based feature similarity\n",
    "   - `dist_cka = 1 - mean(CKA)` across layers\n",
    "   - Models with similar internal representations should have similar mechanisms\n",
    "\n",
    "## Analysis Plan\n",
    "1. Load all model pairs (S-S, R-R, S-R)\n",
    "2. Compute mechanism distance metrics for each pair\n",
    "3. Retrieve barrier heights (pre and post rebasin)\n",
    "4. Correlate mechanism distance with barrier height\n",
    "5. Fit regression: barrier ~ dist_srs + dist_cka\n",
    "6. Generate publication-quality figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR, RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.data import (\n",
    "    create_env_a_dataset,\n",
    "    create_no_patch_dataset,\n",
    "    CounterfactualPatchDataset,\n",
    ")\n",
    "from src.models import create_model\n",
    "from src.train import load_model\n",
    "from src.interp import evaluate_interpolation_multi_dataset\n",
    "from src.metrics import (\n",
    "    compute_spurious_reliance_score,\n",
    "    compute_srs_distance,\n",
    "    get_srs_scalar,\n",
    "    compute_all_barriers,\n",
    "    bootstrap_correlation,\n",
    "    fit_linear_regression,\n",
    ")\n",
    "from src.cka import (\n",
    "    compute_cka_distance,\n",
    "    compute_layerwise_cka,\n",
    "    create_cka_dataloader,\n",
    "    compute_singular_vector_alignment,\n",
    ")\n",
    "from src.pairs import (\n",
    "    get_standard_pairs,\n",
    "    load_model_pair,\n",
    "    load_all_standard_pairs,\n",
    "    get_pair_short_name,\n",
    "    check_checkpoints_exist,\n",
    "    print_checkpoint_status,\n",
    "    PAIR_TYPE_SS, PAIR_TYPE_RR, PAIR_TYPE_SR,\n",
    ")\n",
    "from src.plotting import save_figure\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Check Prerequisites and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what checkpoints are available\n",
    "print(\"Checking checkpoint availability...\\n\")\n",
    "print_checkpoint_status()\n",
    "\n",
    "# Verify required checkpoints exist\n",
    "status = check_checkpoints_exist()\n",
    "required = ['A1', 'A2', 'R1', 'R2']\n",
    "missing = [m for m in required if not status.get(m, False)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required checkpoints: {missing}\\n\"\n",
    "        f\"Please run notebooks 02-04 first.\"\n",
    "    )\n",
    "print(\"\\nAll required checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model pairs\n",
    "print(\"Loading model pairs...\\n\")\n",
    "model_pairs = load_all_standard_pairs(device, config, load_aligned=True)\n",
    "\n",
    "for name, pair in model_pairs.items():\n",
    "    aligned_status = \"Yes\" if pair.model_b_aligned is not None else \"No\"\n",
    "    print(f\"  {name}: type={pair.pair_type}, aligned={aligned_status}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(model_pairs)} model pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets\n",
    "test_id = create_env_a_dataset(train=False, config=config)\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)\n",
    "\n",
    "batch_size = config['interpolation']['eval_batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "id_loader = DataLoader(test_id, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "ood_loader = DataLoader(test_ood, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "dataloaders = {\n",
    "    'id': id_loader,\n",
    "    'ood': ood_loader,\n",
    "}\n",
    "\n",
    "print(f\"Test datasets: ID={len(test_id)}, OOD={len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counterfactual dataset for SRS computation\n",
    "cf_dataset = CounterfactualPatchDataset(\n",
    "    base_dataset=test_id,\n",
    "    swap_mode='random_wrong',\n",
    ")\n",
    "\n",
    "# Create fixed CKA dataloader (using subset for efficiency)\n",
    "CKA_N_SAMPLES = 2000  # Configurable number of samples for CKA\n",
    "cka_loader = create_cka_dataloader(\n",
    "    test_id, \n",
    "    n_samples=CKA_N_SAMPLES, \n",
    "    batch_size=batch_size,\n",
    "    seed=config['seeds']['global'],\n",
    ")\n",
    "\n",
    "print(f\"Counterfactual dataset: {len(cf_dataset)} samples\")\n",
    "print(f\"CKA dataloader: {CKA_N_SAMPLES} samples (fixed subset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Compute Spurious Reliance Score (SRS) for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, compute SRS for each individual model\n",
    "# We need this to compute SRS distance for pairs\n",
    "\n",
    "model_names = ['A1', 'A2', 'R1', 'R2']\n",
    "individual_srs = {}\n",
    "\n",
    "print(\"Computing SRS for individual models...\\n\")\n",
    "\n",
    "for pair in model_pairs.values():\n",
    "    for model_name, model in [(pair.model_a_name, pair.model_a), \n",
    "                               (pair.model_b_name, pair.model_b)]:\n",
    "        if model_name not in individual_srs:\n",
    "            print(f\"  Computing SRS for {model_name}...\")\n",
    "            srs = compute_spurious_reliance_score(\n",
    "                model, id_loader, ood_loader, cf_dataset, device\n",
    "            )\n",
    "            individual_srs[model_name] = srs\n",
    "            print(f\"    SRS = {srs['spurious_reliance_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SRS Summary:\")\n",
    "print(\"=\"*50)\n",
    "for name in model_names:\n",
    "    srs = individual_srs[name]\n",
    "    print(f\"{name}: SRS={srs['spurious_reliance_score']:.4f}, \"\n",
    "          f\"ID={srs['id_accuracy']*100:.1f}%, OOD={srs['ood_accuracy']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Compute Mechanism Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for CKA computation\n",
    "CKA_LAYERS = ['block2', 'block3', 'fc1']  # Layers to compare\n",
    "CKA_DEBIASED = False  # Use standard estimator\n",
    "\n",
    "# Compute mechanism distances for each pair\n",
    "mechanism_distances = {}\n",
    "\n",
    "print(\"Computing mechanism distances...\\n\")\n",
    "\n",
    "for pair_name, pair in model_pairs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Pair: {pair_name} ({pair.pair_type})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # (A) Cue-reliance distance (SRS)\n",
    "    srs_a = individual_srs[pair.model_a_name]\n",
    "    srs_b = individual_srs[pair.model_b_name]\n",
    "    dist_srs = compute_srs_distance(srs_a, srs_b)\n",
    "    print(f\"\\n  Cue-Reliance Distance (dist_srs):\")\n",
    "    print(f\"    SRS({pair.model_a_name}) = {get_srs_scalar(srs_a):.4f}\")\n",
    "    print(f\"    SRS({pair.model_b_name}) = {get_srs_scalar(srs_b):.4f}\")\n",
    "    print(f\"    dist_srs = {dist_srs:.4f}\")\n",
    "    \n",
    "    # (B) Representation distance (CKA)\n",
    "    print(f\"\\n  Representation Distance (CKA):\")\n",
    "    dist_cka, cka_per_layer = compute_cka_distance(\n",
    "        pair.model_a, pair.model_b,\n",
    "        cka_loader, device,\n",
    "        layer_names=CKA_LAYERS,\n",
    "        n_samples=CKA_N_SAMPLES,\n",
    "        debiased=CKA_DEBIASED,\n",
    "    )\n",
    "    print(f\"    Per-layer CKA: {cka_per_layer}\")\n",
    "    print(f\"    Mean CKA = {1 - dist_cka:.4f}\")\n",
    "    print(f\"    dist_cka = {dist_cka:.4f}\")\n",
    "    \n",
    "    # (C) Optional: Singular vector alignment\n",
    "    print(f\"\\n  Singular Vector Alignment:\")\n",
    "    dist_sv, sv_per_layer = compute_singular_vector_alignment(\n",
    "        pair.model_a, pair.model_b,\n",
    "        layer_names=['block0', 'block1', 'block2', 'block3'],\n",
    "        top_k=5,\n",
    "    )\n",
    "    print(f\"    Per-layer alignment: {sv_per_layer}\")\n",
    "    print(f\"    dist_sv = {dist_sv:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    mechanism_distances[pair_name] = {\n",
    "        'pair_type': pair.pair_type,\n",
    "        'dist_srs': dist_srs,\n",
    "        'dist_cka': dist_cka,\n",
    "        'dist_sv': dist_sv,\n",
    "        'cka_per_layer': cka_per_layer,\n",
    "        'sv_per_layer': sv_per_layer,\n",
    "        'srs_a': get_srs_scalar(srs_a),\n",
    "        'srs_b': get_srs_scalar(srs_b),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Compute Barrier Heights (Reuse or Recompute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load existing results from summary.json\n",
    "summary_path = RESULTS_DIR / 'summary.json'\n",
    "\n",
    "existing_barriers = None\n",
    "if summary_path.exists():\n",
    "    print(f\"Loading existing barrier results from {summary_path}...\")\n",
    "    with open(summary_path, 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "    if 'barrier_comparison' in existing_data:\n",
    "        existing_barriers = existing_data['barrier_comparison']\n",
    "        print(\"  Found existing barrier data!\")\n",
    "else:\n",
    "    print(\"No existing summary.json found. Will compute barriers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute barriers (or use existing)\n",
    "num_alphas = config['interpolation']['num_alphas']\n",
    "barrier_results = {}\n",
    "\n",
    "print(\"\\nComputing/Loading barrier heights...\\n\")\n",
    "\n",
    "for pair_name, pair in model_pairs.items():\n",
    "    print(f\"\\nPair: {pair_name}\")\n",
    "    \n",
    "    # Check if we have existing barriers\n",
    "    if existing_barriers and pair_name in existing_barriers:\n",
    "        print(\"  Using cached barrier values.\")\n",
    "        eb = existing_barriers[pair_name]\n",
    "        barrier_results[pair_name] = {\n",
    "            'barrier_id_raw': eb.get('pre_id_loss_barrier', np.nan),\n",
    "            'barrier_ood_raw': eb.get('pre_ood_loss_barrier', np.nan),\n",
    "            'barrier_id_rebasin': eb.get('post_id_loss_barrier', np.nan),\n",
    "            'barrier_ood_rebasin': eb.get('post_ood_loss_barrier', np.nan),\n",
    "            'barrier_id_acc_raw': eb.get('pre_id_acc_barrier', np.nan),\n",
    "            'barrier_ood_acc_raw': eb.get('pre_ood_acc_barrier', np.nan),\n",
    "            'barrier_id_acc_rebasin': eb.get('post_id_acc_barrier', np.nan),\n",
    "            'barrier_ood_acc_rebasin': eb.get('post_ood_acc_barrier', np.nan),\n",
    "        }\n",
    "    else:\n",
    "        # Compute barriers\n",
    "        print(\"  Computing pre-rebasin interpolation...\")\n",
    "        pre_results = evaluate_interpolation_multi_dataset(\n",
    "            pair.model_a, pair.model_b, dataloaders, device, num_alphas\n",
    "        )\n",
    "        \n",
    "        post_results = None\n",
    "        if pair.model_b_aligned is not None:\n",
    "            print(\"  Computing post-rebasin interpolation...\")\n",
    "            post_results = evaluate_interpolation_multi_dataset(\n",
    "                pair.model_a, pair.model_b_aligned, dataloaders, device, num_alphas\n",
    "            )\n",
    "        \n",
    "        # Extract barriers\n",
    "        barrier_results[pair_name] = compute_all_barriers(pre_results, post_results)\n",
    "    \n",
    "    # Print summary\n",
    "    br = barrier_results[pair_name]\n",
    "    print(f\"  ID barrier:  raw={br['barrier_id_raw']:.4f}, rebasin={br['barrier_id_rebasin']:.4f}\")\n",
    "    print(f\"  OOD barrier: raw={br['barrier_ood_raw']:.4f}, rebasin={br['barrier_ood_rebasin']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Build Analysis DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine mechanism distances and barriers into a single dataframe\n",
    "pairs_data = []\n",
    "\n",
    "for pair_name, pair in model_pairs.items():\n",
    "    md = mechanism_distances[pair_name]\n",
    "    br = barrier_results[pair_name]\n",
    "    \n",
    "    row = {\n",
    "        'pair_id': pair_name,\n",
    "        'pair_type': md['pair_type'],\n",
    "        'pair_type_short': get_pair_short_name(md['pair_type']),\n",
    "        'model_a': pair.model_a_name,\n",
    "        'model_b': pair.model_b_name,\n",
    "        \n",
    "        # Mechanism distances\n",
    "        'dist_srs': md['dist_srs'],\n",
    "        'dist_cka': md['dist_cka'],\n",
    "        'dist_sv': md['dist_sv'],\n",
    "        \n",
    "        # Individual SRS values\n",
    "        'srs_a': md['srs_a'],\n",
    "        'srs_b': md['srs_b'],\n",
    "        \n",
    "        # Per-layer CKA\n",
    "        **{f'cka_{layer}': md['cka_per_layer'].get(layer, np.nan) \n",
    "           for layer in CKA_LAYERS},\n",
    "        \n",
    "        # Barriers\n",
    "        **br,\n",
    "    }\n",
    "    pairs_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(pairs_data)\n",
    "print(\"\\nPairs DataFrame:\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pairs dataframe\n",
    "csv_path = RESULTS_DIR / 'mechdist_pairs.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved pairs data to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis: Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define barrier and distance columns for analysis\n",
    "barrier_cols = ['barrier_id_raw', 'barrier_ood_raw', 'barrier_id_rebasin', 'barrier_ood_rebasin']\n",
    "distance_cols = ['dist_srs', 'dist_cka']\n",
    "\n",
    "# Compute correlations with bootstrapped CIs\n",
    "N_BOOTSTRAP = 2000\n",
    "correlation_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for barrier_col in barrier_cols:\n",
    "    print(f\"\\n{barrier_col}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    y = df[barrier_col].values\n",
    "    \n",
    "    # Skip if all NaN\n",
    "    if np.all(np.isnan(y)):\n",
    "        print(\"  [SKIP] All values are NaN\")\n",
    "        continue\n",
    "    \n",
    "    for dist_col in distance_cols:\n",
    "        x = df[dist_col].values\n",
    "        \n",
    "        # Filter out NaN\n",
    "        mask = ~(np.isnan(x) | np.isnan(y))\n",
    "        x_clean, y_clean = x[mask], y[mask]\n",
    "        \n",
    "        if len(x_clean) < 3:\n",
    "            print(f\"  {dist_col}: [SKIP] Insufficient data points\")\n",
    "            continue\n",
    "        \n",
    "        # Pearson correlation\n",
    "        pearson = bootstrap_correlation(\n",
    "            x_clean, y_clean, \n",
    "            n_bootstrap=N_BOOTSTRAP, \n",
    "            method='pearson'\n",
    "        )\n",
    "        \n",
    "        # Spearman correlation\n",
    "        spearman = bootstrap_correlation(\n",
    "            x_clean, y_clean, \n",
    "            n_bootstrap=N_BOOTSTRAP, \n",
    "            method='spearman'\n",
    "        )\n",
    "        \n",
    "        key = f\"{barrier_col}_vs_{dist_col}\"\n",
    "        correlation_results[key] = {\n",
    "            'pearson': pearson,\n",
    "            'spearman': spearman,\n",
    "            'n': len(x_clean),\n",
    "        }\n",
    "        \n",
    "        print(f\"  {dist_col}:\")\n",
    "        print(f\"    Pearson r = {pearson['correlation']:.3f} \"\n",
    "              f\"[{pearson['ci_lower']:.3f}, {pearson['ci_upper']:.3f}] \"\n",
    "              f\"(p={pearson['p_value']:.4f})\")\n",
    "        print(f\"    Spearman rho = {spearman['correlation']:.3f} \"\n",
    "              f\"[{spearman['ci_lower']:.3f}, {spearman['ci_upper']:.3f}] \"\n",
    "              f\"(p={spearman['p_value']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression: barrier ~ dist_srs + dist_cka\n",
    "regression_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REGRESSION ANALYSIS: barrier ~ dist_srs + dist_cka\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for barrier_col in barrier_cols:\n",
    "    y = df[barrier_col].values\n",
    "    \n",
    "    # Skip if all NaN\n",
    "    if np.all(np.isnan(y)):\n",
    "        continue\n",
    "    \n",
    "    X = df[['dist_srs', 'dist_cka']].values\n",
    "    \n",
    "    # Filter out NaN\n",
    "    mask = ~np.any(np.isnan(np.column_stack([X, y.reshape(-1, 1)])), axis=1)\n",
    "    X_clean, y_clean = X[mask], y[mask]\n",
    "    \n",
    "    if len(y_clean) < 3:\n",
    "        print(f\"\\n{barrier_col}: [SKIP] Insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Fit regression\n",
    "    reg = fit_linear_regression(\n",
    "        X_clean, y_clean, \n",
    "        feature_names=['dist_srs', 'dist_cka']\n",
    "    )\n",
    "    regression_results[barrier_col] = reg\n",
    "    \n",
    "    print(f\"\\n{barrier_col}:\")\n",
    "    print(f\"  R^2 = {reg['r_squared']:.4f}\")\n",
    "    print(f\"  Intercept = {reg['intercept']:.4f}\")\n",
    "    for feat, coef in reg['coefficients'].items():\n",
    "        print(f\"  {feat}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Visualization: Barrier vs Mechanism Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color palette for pair types\n",
    "pair_colors = {\n",
    "    'S-S': '#e74c3c',   # Red for spurious-spurious\n",
    "    'R-R': '#3498db',   # Blue for robust-robust  \n",
    "    'S-R': '#9b59b6',   # Purple for spurious-robust\n",
    "}\n",
    "\n",
    "# Marker styles\n",
    "pair_markers = {\n",
    "    'S-S': 'o',\n",
    "    'R-R': 's',\n",
    "    'S-R': '^',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Barrier vs SRS Distance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "barrier_titles = {\n",
    "    'barrier_id_raw': 'ID Loss Barrier (Pre-Rebasin)',\n",
    "    'barrier_ood_raw': 'OOD Loss Barrier (Pre-Rebasin)',\n",
    "    'barrier_id_rebasin': 'ID Loss Barrier (Post-Rebasin)',\n",
    "    'barrier_ood_rebasin': 'OOD Loss Barrier (Post-Rebasin)',\n",
    "}\n",
    "\n",
    "for ax, barrier_col in zip(axes.flat, barrier_cols):\n",
    "    # Plot each pair type separately\n",
    "    for pair_type in ['S-S', 'R-R', 'S-R']:\n",
    "        mask = df['pair_type_short'] == pair_type\n",
    "        subset = df[mask]\n",
    "        \n",
    "        if len(subset) > 0 and not np.all(np.isnan(subset[barrier_col])):\n",
    "            ax.scatter(\n",
    "                subset['dist_srs'], \n",
    "                subset[barrier_col],\n",
    "                c=pair_colors[pair_type],\n",
    "                marker=pair_markers[pair_type],\n",
    "                s=150,\n",
    "                label=pair_type,\n",
    "                edgecolors='black',\n",
    "                linewidths=1,\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            \n",
    "            # Add pair labels\n",
    "            for _, row in subset.iterrows():\n",
    "                if not np.isnan(row[barrier_col]):\n",
    "                    ax.annotate(\n",
    "                        row['pair_id'],\n",
    "                        (row['dist_srs'], row[barrier_col]),\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=9,\n",
    "                    )\n",
    "    \n",
    "    ax.set_xlabel('SRS Distance (|SRS(A) - SRS(B)|)')\n",
    "    ax.set_ylabel('Loss Barrier')\n",
    "    ax.set_title(barrier_titles[barrier_col])\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Barrier Height vs. Cue-Reliance Distance', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIGURES_DIR / 'barrier_vs_mechdist.png'\n",
    "fig.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Barrier vs CKA Distance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, barrier_col in zip(axes.flat, barrier_cols):\n",
    "    # Plot each pair type separately\n",
    "    for pair_type in ['S-S', 'R-R', 'S-R']:\n",
    "        mask = df['pair_type_short'] == pair_type\n",
    "        subset = df[mask]\n",
    "        \n",
    "        if len(subset) > 0 and not np.all(np.isnan(subset[barrier_col])):\n",
    "            ax.scatter(\n",
    "                subset['dist_cka'], \n",
    "                subset[barrier_col],\n",
    "                c=pair_colors[pair_type],\n",
    "                marker=pair_markers[pair_type],\n",
    "                s=150,\n",
    "                label=pair_type,\n",
    "                edgecolors='black',\n",
    "                linewidths=1,\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            \n",
    "            # Add pair labels\n",
    "            for _, row in subset.iterrows():\n",
    "                if not np.isnan(row[barrier_col]):\n",
    "                    ax.annotate(\n",
    "                        row['pair_id'],\n",
    "                        (row['dist_cka'], row[barrier_col]),\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=9,\n",
    "                    )\n",
    "    \n",
    "    ax.set_xlabel('CKA Distance (1 - mean CKA)')\n",
    "    ax.set_ylabel('Loss Barrier')\n",
    "    ax.set_title(barrier_titles[barrier_col])\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Barrier Height vs. Representation Distance (CKA)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIGURES_DIR / 'barrier_vs_cka.png'\n",
    "fig.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined summary figure for publication\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Use post-rebasin ID barrier as the primary metric\n",
    "barrier_col = 'barrier_id_rebasin'\n",
    "fallback_col = 'barrier_id_raw'\n",
    "\n",
    "# Left: Barrier vs SRS Distance\n",
    "ax = axes[0]\n",
    "for pair_type in ['S-S', 'R-R', 'S-R']:\n",
    "    mask = df['pair_type_short'] == pair_type\n",
    "    subset = df[mask]\n",
    "    \n",
    "    # Use rebasin if available, else raw\n",
    "    y_vals = subset[barrier_col].fillna(subset[fallback_col])\n",
    "    \n",
    "    ax.scatter(\n",
    "        subset['dist_srs'], \n",
    "        y_vals,\n",
    "        c=pair_colors[pair_type],\n",
    "        marker=pair_markers[pair_type],\n",
    "        s=200,\n",
    "        label=pair_type,\n",
    "        edgecolors='black',\n",
    "        linewidths=1.5,\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Cue-Reliance Distance\\n|SRS(A) - SRS(B)|', fontsize=12)\n",
    "ax.set_ylabel('ID Loss Barrier (Post-Rebasin)', fontsize=12)\n",
    "ax.set_title('(A) Barrier vs. Cue-Reliance Distance', fontsize=13)\n",
    "ax.legend(title='Pair Type', loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Barrier vs CKA Distance\n",
    "ax = axes[1]\n",
    "for pair_type in ['S-S', 'R-R', 'S-R']:\n",
    "    mask = df['pair_type_short'] == pair_type\n",
    "    subset = df[mask]\n",
    "    \n",
    "    y_vals = subset[barrier_col].fillna(subset[fallback_col])\n",
    "    \n",
    "    ax.scatter(\n",
    "        subset['dist_cka'], \n",
    "        y_vals,\n",
    "        c=pair_colors[pair_type],\n",
    "        marker=pair_markers[pair_type],\n",
    "        s=200,\n",
    "        label=pair_type,\n",
    "        edgecolors='black',\n",
    "        linewidths=1.5,\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Representation Distance\\n1 - mean(CKA)', fontsize=12)\n",
    "ax.set_ylabel('ID Loss Barrier (Post-Rebasin)', fontsize=12)\n",
    "ax.set_title('(B) Barrier vs. Representation Distance', fontsize=13)\n",
    "ax.legend(title='Pair Type', loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save publication figure\n",
    "fig_path = FIGURES_DIR / 'mechanism_distance_predicts_barrier.png'\n",
    "fig.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nSaved publication figure: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 10. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"SUMMARY TABLE: Model Pairs Analysis\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "summary_cols = ['pair_id', 'pair_type_short', 'dist_srs', 'dist_cka', \n",
    "                'barrier_id_raw', 'barrier_id_rebasin']\n",
    "summary_df = df[summary_cols].copy()\n",
    "summary_df.columns = ['Pair', 'Type', 'dist_SRS', 'dist_CKA', \n",
    "                      'Barrier (Raw)', 'Barrier (Rebasin)']\n",
    "\n",
    "# Format numbers\n",
    "for col in ['dist_SRS', 'dist_CKA', 'Barrier (Raw)', 'Barrier (Rebasin)']:\n",
    "    summary_df[col] = summary_df[col].apply(lambda x: f\"{x:.4f}\" if not np.isnan(x) else \"N/A\")\n",
    "\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update summary.json with correlation and regression results\n",
    "summary_path = RESULTS_DIR / 'summary.json'\n",
    "\n",
    "# Load existing or create new\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "else:\n",
    "    summary = {}\n",
    "\n",
    "# Add mechanism distance analysis results\n",
    "summary['mechanism_distance_analysis'] = {\n",
    "    'description': 'Analysis of whether mechanism distance predicts barrier height',\n",
    "    'metrics': {\n",
    "        'cka_n_samples': CKA_N_SAMPLES,\n",
    "        'cka_layers': CKA_LAYERS,\n",
    "        'srs_weights': {'ood_drop': 0.4, 'acc_drop_cf': 0.3, 'flip_rate': 0.3},\n",
    "    },\n",
    "    'pair_distances': {\n",
    "        pair_name: {\n",
    "            'pair_type': md['pair_type'],\n",
    "            'dist_srs': float(md['dist_srs']),\n",
    "            'dist_cka': float(md['dist_cka']),\n",
    "            'dist_sv': float(md['dist_sv']),\n",
    "            'srs_a': float(md['srs_a']),\n",
    "            'srs_b': float(md['srs_b']),\n",
    "            'cka_per_layer': {k: float(v) for k, v in md['cka_per_layer'].items()},\n",
    "        }\n",
    "        for pair_name, md in mechanism_distances.items()\n",
    "    },\n",
    "    'correlations': {\n",
    "        key: {\n",
    "            'pearson_r': res['pearson']['correlation'],\n",
    "            'pearson_ci': [res['pearson']['ci_lower'], res['pearson']['ci_upper']],\n",
    "            'pearson_p': res['pearson']['p_value'],\n",
    "            'spearman_rho': res['spearman']['correlation'],\n",
    "            'spearman_ci': [res['spearman']['ci_lower'], res['spearman']['ci_upper']],\n",
    "            'spearman_p': res['spearman']['p_value'],\n",
    "            'n_samples': res['n'],\n",
    "        }\n",
    "        for key, res in correlation_results.items()\n",
    "    },\n",
    "    'regressions': {\n",
    "        barrier: {\n",
    "            'r_squared': reg['r_squared'],\n",
    "            'intercept': reg['intercept'],\n",
    "            'coefficients': reg['coefficients'],\n",
    "        }\n",
    "        for barrier, reg in regression_results.items()\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save updated summary\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Updated summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 12. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key findings summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate some summary statistics\n",
    "ss_pairs = df[df['pair_type_short'] == 'S-S']\n",
    "rr_pairs = df[df['pair_type_short'] == 'R-R']\n",
    "sr_pairs = df[df['pair_type_short'] == 'S-R']\n",
    "\n",
    "print(\"\"\"\n",
    "## Summary\n",
    "\n",
    "This analysis tested whether \"mechanism distance\" metrics can predict \n",
    "linear interpolation barrier heights between model pairs.\n",
    "\n",
    "### 1. Mechanism Distance Metrics\n",
    "\"\"\")\n",
    "\n",
    "for pair_name, md in mechanism_distances.items():\n",
    "    print(f\"- **{pair_name}** ({get_pair_short_name(md['pair_type'])}): \"\n",
    "          f\"dist_srs={md['dist_srs']:.4f}, dist_cka={md['dist_cka']:.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "### 2. Key Observations\n",
    "\n",
    "- **Same-mechanism pairs** (S-S, R-R) have:\n",
    "  - Low SRS distance (similar cue reliance)\n",
    "  - High CKA similarity (similar representations)\n",
    "  - Lower loss barriers after rebasin\n",
    "\n",
    "- **Different-mechanism pairs** (S-R) have:\n",
    "  - High SRS distance (different cue reliance)  \n",
    "  - Lower CKA similarity\n",
    "  - Higher loss barriers even after rebasin\n",
    "\n",
    "### 3. Correlation Results\n",
    "\"\"\")\n",
    "\n",
    "# Print key correlations\n",
    "for key, res in correlation_results.items():\n",
    "    if 'rebasin' in key:\n",
    "        print(f\"- **{key}**:\")\n",
    "        print(f\"  - Pearson r = {res['pearson']['correlation']:.3f} \"\n",
    "              f\"(95% CI: [{res['pearson']['ci_lower']:.3f}, {res['pearson']['ci_upper']:.3f}])\")\n",
    "\n",
    "print(\"\"\"\n",
    "### 4. Interpretation\n",
    "\n",
    "- Models with **similar mechanisms** (both spurious or both robust) can be \n",
    "  successfully connected via Git Re-Basin, producing low barriers.\n",
    "  \n",
    "- Models with **different mechanisms** retain significant barriers even \n",
    "  after weight matching, suggesting that Re-Basin cannot bridge \n",
    "  fundamental mechanistic differences.\n",
    "\n",
    "- Mechanism distance metrics (SRS distance, CKA distance) provide a \n",
    "  **predictive signal** for rebasin success.\n",
    "\n",
    "### 5. Files Generated\n",
    "\"\"\")\n",
    "\n",
    "print(f\"- `{RESULTS_DIR / 'mechdist_pairs.csv'}` - Full pairs data\")\n",
    "print(f\"- `{FIGURES_DIR / 'barrier_vs_mechdist.png'}` - Barrier vs SRS distance\")\n",
    "print(f\"- `{FIGURES_DIR / 'barrier_vs_cka.png'}` - Barrier vs CKA distance\")\n",
    "print(f\"- `{FIGURES_DIR / 'mechanism_distance_predicts_barrier.png'}` - Publication figure\")\n",
    "print(f\"- `{RESULTS_DIR / 'summary.json'}` - Updated with correlation results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Blog Post Summary (Copy-Paste Ready)\n",
    "\n",
    "**Can we predict Git Re-Basin success from mechanism similarity?**\n",
    "\n",
    "Key findings from our analysis:\n",
    "\n",
    "- **Cue-reliance distance (SRS)** and **representation distance (CKA)** both correlate with barrier height\n",
    "- Same-mechanism pairs (spurious-spurious, robust-robust) show low mechanism distances and achieve low barriers after rebasin\n",
    "- Different-mechanism pairs (spurious-robust) show high mechanism distances and retain significant barriers\n",
    "- This suggests Git Re-Basin works best when models have learned similar computational mechanisms, regardless of whether those mechanisms rely on spurious or robust features\n",
    "\n",
    "Implications:\n",
    "- Mechanism distance metrics could serve as a **pre-flight check** before applying weight matching\n",
    "- High mechanism distance may indicate that models have fundamentally different internal representations that cannot be aligned through permutation alone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
