{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Train Models\n",
    "\n",
    "This notebook trains the 4 models needed for the experiment:\n",
    "\n",
    "1. **Model A1** (Spurious, seed 1): ERM on Env A only\n",
    "2. **Model A2** (Spurious, seed 2): ERM on Env A only\n",
    "3. **Model R1** (Robust, seed 1): ERM on mixed Env A + Env B\n",
    "4. **Model R2** (Robust, seed 2): ERM on mixed Env A + Env B\n",
    "\n",
    "## Expected behavior:\n",
    "- Spurious models (A1, A2) will achieve high ID accuracy but low OOD accuracy\n",
    "- Robust models (R1, R2) will have similar ID and OOD accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    create_env_a_dataset,\n",
    "    create_no_patch_dataset,\n",
    "    create_mixed_env_dataset,\n",
    "    get_dataloaders,\n",
    ")\n",
    "from src.models import create_model, count_parameters\n",
    "from src.train import Trainer, save_training_history\n",
    "from src.plotting import plot_training_curves, plot_multiple_training_curves, save_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets (same for all models)\n",
    "test_id = create_env_a_dataset(train=False, config=config)  # ID test\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)  # OOD test\n",
    "\n",
    "# Create training datasets\n",
    "train_spurious = create_env_a_dataset(train=True, config=config)  # For A1, A2\n",
    "train_robust = create_mixed_env_dataset(env_a_fraction=0.5, train=True, config=config)  # For R1, R2\n",
    "\n",
    "print(\"Datasets created:\")\n",
    "print(f\"  Spurious training (Env A): {len(train_spurious)} samples\")\n",
    "print(f\"  Robust training (Mixed): {len(train_robust)} samples\")\n",
    "print(f\"  ID test: {len(test_id)} samples\")\n",
    "print(f\"  OOD test: {len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "loaders_spurious = get_dataloaders(train_spurious, test_id, test_ood, config)\n",
    "loaders_robust = get_dataloaders(train_robust, test_id, test_ood, config)\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, seed, dataloaders, config, device):\n",
    "    \"\"\"\n",
    "    Train a single model and save checkpoint + history.\n",
    "    \n",
    "    Args:\n",
    "        name: Model name (e.g., 'A1', 'R1')\n",
    "        seed: Random seed for this model\n",
    "        dataloaders: Dictionary with 'train', 'test_id', 'test_ood' loaders\n",
    "        config: Configuration dictionary\n",
    "        device: Torch device\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Model {name} (seed={seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(model, device, config)\n",
    "    \n",
    "    # Train\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    history = trainer.train(\n",
    "        train_loader=dataloaders['train'],\n",
    "        test_id_loader=dataloaders['test_id'],\n",
    "        test_ood_loader=dataloaders['test_ood'],\n",
    "        num_epochs=config['training']['num_epochs'],\n",
    "        verbose=True,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history_path = METRICS_DIR / f\"history_{name}.json\"\n",
    "    save_training_history(history, history_path)\n",
    "    \n",
    "    print(f\"\\nModel {name} training complete!\")\n",
    "    print(f\"  Final ID accuracy: {history['id_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"  Final OOD accuracy: {history['ood_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"  Checkpoint saved to: {checkpoint_path}\")\n",
    "    print(f\"  History saved to: {history_path}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Spurious Model A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A1, history_A1 = train_model(\n",
    "    name='A1',\n",
    "    seed=config['seeds']['model_A1'],\n",
    "    dataloaders=loaders_spurious,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Spurious Model A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A2, history_A2 = train_model(\n",
    "    name='A2',\n",
    "    seed=config['seeds']['model_A2'],\n",
    "    dataloaders=loaders_spurious,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Robust Model R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_R1, history_R1 = train_model(\n",
    "    name='R1',\n",
    "    seed=config['seeds']['model_R1'],\n",
    "    dataloaders=loaders_robust,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Robust Model R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_R2, history_R2 = train_model(\n",
    "    name='R2',\n",
    "    seed=config['seeds']['model_R2'],\n",
    "    dataloaders=loaders_robust,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual training curves for each model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plot_training_curves(history_A1, title=\"Model A1 (Spurious, seed 1)\", save_name='training_A1')\n",
    "plt.show()\n",
    "\n",
    "fig = plot_training_curves(history_A2, title=\"Model A2 (Spurious, seed 2)\", save_name='training_A2')\n",
    "plt.show()\n",
    "\n",
    "fig = plot_training_curves(history_R1, title=\"Model R1 (Robust, seed 1)\", save_name='training_R1')\n",
    "plt.show()\n",
    "\n",
    "fig = plot_training_curves(history_R2, title=\"Model R2 (Robust, seed 2)\", save_name='training_R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plots\n",
    "histories = {\n",
    "    'A1 (Spurious)': history_A1,\n",
    "    'A2 (Spurious)': history_A2,\n",
    "    'R1 (Robust)': history_R1,\n",
    "    'R2 (Robust)': history_R2,\n",
    "}\n",
    "\n",
    "# ID accuracy comparison\n",
    "fig = plot_multiple_training_curves(\n",
    "    histories, \n",
    "    metric='id_acc', \n",
    "    title='ID Test Accuracy Comparison',\n",
    "    save_name='comparison_id_acc'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# OOD accuracy comparison\n",
    "fig = plot_multiple_training_curves(\n",
    "    histories, \n",
    "    metric='ood_acc', \n",
    "    title='OOD Test Accuracy Comparison',\n",
    "    save_name='comparison_ood_acc'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined comparison figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'A1': 'tab:red', 'A2': 'tab:orange', 'R1': 'tab:blue', 'R2': 'tab:green'}\n",
    "linestyles = {'A1': '-', 'A2': '--', 'R1': '-', 'R2': '--'}\n",
    "\n",
    "for name, history in [('A1', history_A1), ('A2', history_A2), ('R1', history_R1), ('R2', history_R2)]:\n",
    "    epochs = range(1, len(history['id_acc']) + 1)\n",
    "    \n",
    "    axes[0].plot(epochs, [x*100 for x in history['id_acc']], \n",
    "                 label=name, color=colors[name], linestyle=linestyles[name], linewidth=2)\n",
    "    axes[1].plot(epochs, [x*100 for x in history['ood_acc']], \n",
    "                 label=name, color=colors[name], linestyle=linestyles[name], linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('ID Test Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('OOD Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Progress: Spurious vs Robust Models', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'training_comparison_combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display summary statistics\n",
    "summary = {}\n",
    "\n",
    "for name, history in [('A1', history_A1), ('A2', history_A2), ('R1', history_R1), ('R2', history_R2)]:\n",
    "    final_id_acc = history['id_acc'][-1]\n",
    "    final_ood_acc = history['ood_acc'][-1]\n",
    "    ood_drop = final_id_acc - final_ood_acc\n",
    "    \n",
    "    summary[name] = {\n",
    "        'id_acc': final_id_acc,\n",
    "        'ood_acc': final_ood_acc,\n",
    "        'ood_drop': ood_drop,\n",
    "        'final_train_loss': history['train_loss'][-1],\n",
    "    }\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10} {'ID Acc':<12} {'OOD Acc':<12} {'OOD Drop':<12} {'Train Loss':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, stats in summary.items():\n",
    "    model_type = \"Spurious\" if name.startswith('A') else \"Robust\"\n",
    "    print(f\"{name} ({model_type[0]})   {stats['id_acc']*100:>6.2f}%      {stats['ood_acc']*100:>6.2f}%      \"\n",
    "          f\"{stats['ood_drop']*100:>+6.2f}%      {stats['final_train_loss']:.4f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify expected behavior\n",
    "print(\"\\nVerification:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Spurious models should have large OOD drop\n",
    "spurious_avg_drop = (summary['A1']['ood_drop'] + summary['A2']['ood_drop']) / 2\n",
    "robust_avg_drop = (summary['R1']['ood_drop'] + summary['R2']['ood_drop']) / 2\n",
    "\n",
    "print(f\"Average OOD drop (Spurious A1, A2): {spurious_avg_drop*100:.2f}%\")\n",
    "print(f\"Average OOD drop (Robust R1, R2):   {robust_avg_drop*100:.2f}%\")\n",
    "\n",
    "if spurious_avg_drop > robust_avg_drop + 0.1:\n",
    "    print(\"\\n[PASS] Spurious models show significantly larger OOD drop!\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] OOD drop difference is smaller than expected.\")\n",
    "    print(\"          This may affect the semantic barrier analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "summary_path = METRICS_DIR / 'training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Models trained:\n",
    "\n",
    "Spurious Models (trained on Env A only):\n",
    "  - A1: ID={summary['A1']['id_acc']*100:.1f}%, OOD={summary['A1']['ood_acc']*100:.1f}%, Drop={summary['A1']['ood_drop']*100:+.1f}%\n",
    "  - A2: ID={summary['A2']['id_acc']*100:.1f}%, OOD={summary['A2']['ood_acc']*100:.1f}%, Drop={summary['A2']['ood_drop']*100:+.1f}%\n",
    "\n",
    "Robust Models (trained on mixed Env A + B):\n",
    "  - R1: ID={summary['R1']['id_acc']*100:.1f}%, OOD={summary['R1']['ood_acc']*100:.1f}%, Drop={summary['R1']['ood_drop']*100:+.1f}%\n",
    "  - R2: ID={summary['R2']['id_acc']*100:.1f}%, OOD={summary['R2']['ood_acc']*100:.1f}%, Drop={summary['R2']['ood_drop']*100:+.1f}%\n",
    "\n",
    "Checkpoints saved:\n",
    "  - {CHECKPOINTS_DIR / 'model_A1.pt'}\n",
    "  - {CHECKPOINTS_DIR / 'model_A2.pt'}\n",
    "  - {CHECKPOINTS_DIR / 'model_R1.pt'}\n",
    "  - {CHECKPOINTS_DIR / 'model_R2.pt'}\n",
    "\n",
    "Training histories saved:\n",
    "  - {METRICS_DIR / 'history_A1.json'}\n",
    "  - {METRICS_DIR / 'history_A2.json'}\n",
    "  - {METRICS_DIR / 'history_R1.json'}\n",
    "  - {METRICS_DIR / 'history_R2.json'}\n",
    "\n",
    "Figures saved:\n",
    "  - Training curves for each model\n",
    "  - Comparison plots\n",
    "\n",
    "Next: Run 03_mechanism_verification.ipynb to quantify spurious reliance.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
