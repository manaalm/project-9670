{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Interpolation and Loss Barriers\n",
    "\n",
    "This notebook analyzes loss barriers along weight interpolation paths between model pairs.\n",
    "\n",
    "## Core Hypothesis:\n",
    "- **Same-mechanism pairs** (spurious-spurious, robust-robust): Should have LOW barriers after rebasin\n",
    "- **Different-mechanism pairs** (spurious-robust): Should have HIGH barriers even after rebasin\n",
    "\n",
    "## What this notebook does:\n",
    "1. Interpolates weights: θ(α) = α·θ₁ + (1-α)·θ₂ for α ∈ [0, 1]\n",
    "2. Evaluates ID loss/acc and OOD loss/acc at each α\n",
    "3. Computes Spurious Reliance Score along the path\n",
    "4. Calculates barrier heights pre and post rebasing\n",
    "5. Generates visualization and summary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import (\n",
    "    get_config, set_seed, get_device,\n",
    "    CHECKPOINTS_DIR, FIGURES_DIR, METRICS_DIR, RESULTS_DIR\n",
    ")\n",
    "\n",
    "config = get_config()\n",
    "set_seed(config['seeds']['global'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    create_env_a_dataset, \n",
    "    create_no_patch_dataset,\n",
    "    CounterfactualPatchDataset,\n",
    ")\n",
    "from src.models import create_model\n",
    "from src.train import load_model\n",
    "from src.interp import (\n",
    "    evaluate_interpolation_path,\n",
    "    evaluate_interpolation_multi_dataset,\n",
    "    compute_loss_barrier,\n",
    "    compute_accuracy_barrier,\n",
    "    summarize_interpolation_results,\n",
    "    compare_pre_post_rebasin,\n",
    ")\n",
    "from src.metrics import compute_spurious_reliance_score, semantic_barrier_metric\n",
    "from src.plotting import (\n",
    "    plot_interpolation_path,\n",
    "    plot_interpolation_comparison,\n",
    "    plot_pre_post_rebasin,\n",
    "    plot_barrier_comparison,\n",
    "    plot_srs_interpolation,\n",
    "    save_figure,\n",
    ")\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models (Original and Aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original models\n",
    "model_names = ['A1', 'A2', 'R1', 'R2']\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{name}.pt\"\n",
    "    model = create_model(config)\n",
    "    model = load_model(model, checkpoint_path, device)\n",
    "    models[name] = model\n",
    "    print(f\"Loaded original model {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aligned models\n",
    "aligned_pairs = [\n",
    "    ('A1', 'A2'),  # A2 aligned to A1\n",
    "    ('R1', 'R2'),  # R2 aligned to R1\n",
    "    ('A1', 'R1'),  # R1 aligned to A1\n",
    "]\n",
    "\n",
    "aligned_models = {}\n",
    "\n",
    "for ref, aligned in aligned_pairs:\n",
    "    pair_name = f\"{ref}-{aligned}\"\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"model_{aligned}_aligned_to_{ref}.pt\"\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        model = create_model(config)\n",
    "        model = load_model(model, checkpoint_path, device)\n",
    "        aligned_models[pair_name] = model\n",
    "        print(f\"Loaded aligned model: {aligned} -> {ref}\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Aligned model not found: {checkpoint_path}\")\n",
    "        print(f\"          Please run 04_rebasin_alignment.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets\n",
    "test_id = create_env_a_dataset(train=False, config=config)\n",
    "test_ood = create_no_patch_dataset(train=False, config=config)\n",
    "\n",
    "batch_size = config['interpolation']['eval_batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "\n",
    "id_loader = DataLoader(test_id, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "ood_loader = DataLoader(test_ood, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "dataloaders = {\n",
    "    'id': id_loader,\n",
    "    'ood': ood_loader,\n",
    "}\n",
    "\n",
    "print(f\"DataLoaders created: ID={len(test_id)}, OOD={len(test_ood)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counterfactual dataset for SRS computation\n",
    "cf_dataset = CounterfactualPatchDataset(\n",
    "    base_dataset=test_id,\n",
    "    swap_mode='random_wrong',\n",
    ")\n",
    "print(f\"Counterfactual dataset: {len(cf_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Interpolation Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pairs to analyze\n",
    "# Format: (ref_name, other_name, pair_type)\n",
    "analysis_pairs = [\n",
    "    ('A1', 'A2', 'spurious-spurious'),\n",
    "    ('R1', 'R2', 'robust-robust'),\n",
    "    ('A1', 'R1', 'spurious-robust'),\n",
    "]\n",
    "\n",
    "num_alphas = config['interpolation']['num_alphas']\n",
    "print(f\"Will evaluate {num_alphas} interpolation points for each pair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Interpolation Paths (Pre and Post Rebasin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "for ref_name, other_name, pair_type in analysis_pairs:\n",
    "    pair_name = f\"{ref_name}-{other_name}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing pair: {pair_name} ({pair_type})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get models\n",
    "    model_ref = models[ref_name]\n",
    "    model_other = models[other_name]\n",
    "    model_other_aligned = aligned_models.get(pair_name)\n",
    "    \n",
    "    # Pre-rebasin interpolation\n",
    "    print(f\"\\nPre-rebasin interpolation...\")\n",
    "    pre_results = evaluate_interpolation_multi_dataset(\n",
    "        model_ref, model_other, dataloaders, device, num_alphas\n",
    "    )\n",
    "    \n",
    "    # Post-rebasin interpolation (if aligned model exists)\n",
    "    if model_other_aligned is not None:\n",
    "        print(f\"Post-rebasin interpolation...\")\n",
    "        post_results = evaluate_interpolation_multi_dataset(\n",
    "            model_ref, model_other_aligned, dataloaders, device, num_alphas\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[SKIP] No aligned model available\")\n",
    "        post_results = None\n",
    "    \n",
    "    # Store results\n",
    "    all_results[pair_name] = {\n",
    "        'type': pair_type,\n",
    "        'pre_rebasin': pre_results,\n",
    "        'post_rebasin': post_results,\n",
    "    }\n",
    "    \n",
    "    # Quick summary\n",
    "    pre_summary = summarize_interpolation_results(pre_results)\n",
    "    print(f\"\\nPre-rebasin barriers:\")\n",
    "    print(f\"  ID loss barrier:  {pre_summary['id']['loss_barrier']:.4f}\")\n",
    "    print(f\"  OOD loss barrier: {pre_summary['ood']['loss_barrier']:.4f}\")\n",
    "    print(f\"  ID acc barrier:   {pre_summary['id']['acc_barrier']*100:.2f}%\")\n",
    "    \n",
    "    if post_results is not None:\n",
    "        post_summary = summarize_interpolation_results(post_results)\n",
    "        print(f\"\\nPost-rebasin barriers:\")\n",
    "        print(f\"  ID loss barrier:  {post_summary['id']['loss_barrier']:.4f}\")\n",
    "        print(f\"  OOD loss barrier: {post_summary['ood']['loss_barrier']:.4f}\")\n",
    "        print(f\"  ID acc barrier:   {post_summary['id']['acc_barrier']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Interpolation Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each pair's interpolation path\n",
    "for pair_name, results in all_results.items():\n",
    "    pair_type = results['type']\n",
    "    pre = results['pre_rebasin']\n",
    "    post = results['post_rebasin']\n",
    "    \n",
    "    # Pre-rebasin path\n",
    "    fig = plot_interpolation_path(\n",
    "        {'alphas': pre['id']['alphas'], 'losses': pre['id']['losses'], 'accuracies': pre['id']['accuracies']},\n",
    "        title=f\"{pair_name} ({pair_type}) - Pre-Rebasin (ID)\",\n",
    "        save_name=f'interp_{pair_name}_pre_id'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Post-rebasin path (if available)\n",
    "    if post is not None:\n",
    "        fig = plot_pre_post_rebasin(\n",
    "            pre['id'], post['id'],\n",
    "            dataset_name=\"ID\",\n",
    "            title=f\"{pair_name} ({pair_type}): Pre vs Post Rebasin (ID)\",\n",
    "            save_name=f'interp_{pair_name}_pre_vs_post_id'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plot_pre_post_rebasin(\n",
    "            pre['ood'], post['ood'],\n",
    "            dataset_name=\"OOD\",\n",
    "            title=f\"{pair_name} ({pair_type}): Pre vs Post Rebasin (OOD)\",\n",
    "            save_name=f'interp_{pair_name}_pre_vs_post_ood'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Barriers Across Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile barrier comparison data\n",
    "barrier_comparison = {}\n",
    "\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    post = results['post_rebasin']\n",
    "    \n",
    "    pre_summary = summarize_interpolation_results(pre)\n",
    "    \n",
    "    barrier_comparison[pair_name] = {\n",
    "        'type': results['type'],\n",
    "        'pre_id_loss_barrier': pre_summary['id']['loss_barrier'],\n",
    "        'pre_ood_loss_barrier': pre_summary['ood']['loss_barrier'],\n",
    "        'pre_id_acc_barrier': pre_summary['id']['acc_barrier'],\n",
    "        'pre_ood_acc_barrier': pre_summary['ood']['acc_barrier'],\n",
    "    }\n",
    "    \n",
    "    if post is not None:\n",
    "        post_summary = summarize_interpolation_results(post)\n",
    "        barrier_comparison[pair_name].update({\n",
    "            'post_id_loss_barrier': post_summary['id']['loss_barrier'],\n",
    "            'post_ood_loss_barrier': post_summary['ood']['loss_barrier'],\n",
    "            'post_id_acc_barrier': post_summary['id']['acc_barrier'],\n",
    "            'post_ood_acc_barrier': post_summary['ood']['acc_barrier'],\n",
    "        })\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"BARRIER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\n{'Pair':<20} {'Type':<18} {'Pre ID Loss':<12} {'Post ID Loss':<12} {'Pre OOD Loss':<12} {'Post OOD Loss':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for pair_name, data in barrier_comparison.items():\n",
    "    pre_id = data['pre_id_loss_barrier']\n",
    "    post_id = data.get('post_id_loss_barrier', float('nan'))\n",
    "    pre_ood = data['pre_ood_loss_barrier']\n",
    "    post_ood = data.get('post_ood_loss_barrier', float('nan'))\n",
    "    \n",
    "    print(f\"{pair_name:<20} {data['type']:<18} {pre_id:>10.4f}   {post_id:>10.4f}   {pre_ood:>10.4f}   {post_ood:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create barrier comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "pair_names = list(barrier_comparison.keys())\n",
    "x = np.arange(len(pair_names))\n",
    "width = 0.35\n",
    "\n",
    "# ID Loss Barrier\n",
    "pre_id_barriers = [barrier_comparison[p]['pre_id_loss_barrier'] for p in pair_names]\n",
    "post_id_barriers = [barrier_comparison[p].get('post_id_loss_barrier', 0) for p in pair_names]\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, pre_id_barriers, width, label='Pre-Rebasin', color='salmon')\n",
    "bars2 = axes[0].bar(x + width/2, post_id_barriers, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Loss Barrier')\n",
    "axes[0].set_title('ID Loss Barrier')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# OOD Loss Barrier\n",
    "pre_ood_barriers = [barrier_comparison[p]['pre_ood_loss_barrier'] for p in pair_names]\n",
    "post_ood_barriers = [barrier_comparison[p].get('post_ood_loss_barrier', 0) for p in pair_names]\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, pre_ood_barriers, width, label='Pre-Rebasin', color='salmon')\n",
    "bars2 = axes[1].bar(x + width/2, post_ood_barriers, width, label='Post-Rebasin', color='steelblue')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Loss Barrier')\n",
    "axes[1].set_title('OOD Loss Barrier')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Loss Barriers: Pre vs Post Re-Basin', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'barrier_comparison_all')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Spurious Reliance Score Along Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interp import create_interpolated_model\n",
    "\n",
    "def compute_srs_along_path(model_a, model_b, alphas, device, id_loader, ood_loader, cf_dataset):\n",
    "    \"\"\"\n",
    "    Compute Spurious Reliance Score at each interpolation point.\n",
    "    \n",
    "    This is expensive - we'll use fewer points.\n",
    "    \"\"\"\n",
    "    srs_values = []\n",
    "    \n",
    "    # Use fewer points for SRS (expensive to compute)\n",
    "    sample_alphas = alphas[::4]  # Every 4th point\n",
    "    \n",
    "    for alpha in sample_alphas:\n",
    "        interp_model = create_interpolated_model(model_a, model_b, alpha, device)\n",
    "        \n",
    "        srs = compute_spurious_reliance_score(\n",
    "            interp_model, id_loader, ood_loader, cf_dataset, device\n",
    "        )\n",
    "        srs_values.append(srs['spurious_reliance_score'])\n",
    "        print(f\"  alpha={alpha:.2f}: SRS={srs['spurious_reliance_score']:.4f}\")\n",
    "    \n",
    "    return sample_alphas, srs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SRS for spurious-robust pair (most interesting case)\n",
    "print(\"Computing Spurious Reliance Score along A1-R1 interpolation path...\")\n",
    "print(\"(This demonstrates the 'semantic barrier' - mechanism mismatch)\\n\")\n",
    "\n",
    "pair_name = 'A1-R1'\n",
    "model_a1 = models['A1']\n",
    "model_r1_aligned = aligned_models.get(pair_name, models['R1'])\n",
    "\n",
    "alphas = np.linspace(0, 1, num_alphas)\n",
    "srs_alphas, srs_values = compute_srs_along_path(\n",
    "    model_a1, model_r1_aligned, alphas, device,\n",
    "    id_loader, ood_loader, cf_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SRS along interpolation\n",
    "fig = plot_srs_interpolation(\n",
    "    np.array(srs_alphas), srs_values,\n",
    "    title=f\"Spurious Reliance Score: A1-R1 Interpolation (Post-Rebasin)\",\n",
    "    save_name='srs_interpolation_A1_R1'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compute semantic barrier\n",
    "sem_barrier, sem_alpha = semantic_barrier_metric(srs_values, np.array(srs_alphas))\n",
    "print(f\"\\nSemantic Barrier Metric:\")\n",
    "print(f\"  Max SRS variation from endpoint average: {sem_barrier:.4f}\")\n",
    "print(f\"  At alpha = {sem_alpha:.2f}\")\n",
    "print(f\"\\nEndpoint SRS values:\")\n",
    "print(f\"  A1 (alpha=1): SRS = {srs_values[-1]:.4f} (should be HIGH - spurious)\")\n",
    "print(f\"  R1 (alpha=0): SRS = {srs_values[0]:.4f} (should be LOW - robust)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Combined Interpolation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "colors = {'A1-A2': 'tab:red', 'R1-R2': 'tab:blue', 'A1-R1': 'tab:purple'}\n",
    "\n",
    "# Row 1: Pre-rebasin\n",
    "# Loss (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    axes[0, 0].plot(pre['id']['alphas'], pre['id']['losses'], \n",
    "                    label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[0, 0].set_xlabel(r'$\\alpha$')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Pre-Rebasin: ID Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    axes[0, 1].plot(pre['id']['alphas'], pre['id']['accuracies']*100, \n",
    "                    label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[0, 1].set_xlabel(r'$\\alpha$')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Pre-Rebasin: ID Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (OOD)\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    axes[0, 2].plot(pre['ood']['alphas'], pre['ood']['accuracies']*100, \n",
    "                    label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[0, 2].set_xlabel(r'$\\alpha$')\n",
    "axes[0, 2].set_ylabel('Accuracy (%)')\n",
    "axes[0, 2].set_title('Pre-Rebasin: OOD Accuracy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Post-rebasin\n",
    "# Loss (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    post = results['post_rebasin']\n",
    "    if post is not None:\n",
    "        axes[1, 0].plot(post['id']['alphas'], post['id']['losses'], \n",
    "                        label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[1, 0].set_xlabel(r'$\\alpha$')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Post-Rebasin: ID Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (ID)\n",
    "for pair_name, results in all_results.items():\n",
    "    post = results['post_rebasin']\n",
    "    if post is not None:\n",
    "        axes[1, 1].plot(post['id']['alphas'], post['id']['accuracies']*100, \n",
    "                        label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[1, 1].set_xlabel(r'$\\alpha$')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_title('Post-Rebasin: ID Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy (OOD)\n",
    "for pair_name, results in all_results.items():\n",
    "    post = results['post_rebasin']\n",
    "    if post is not None:\n",
    "        axes[1, 2].plot(post['ood']['alphas'], post['ood']['accuracies']*100, \n",
    "                        label=f\"{pair_name}\", color=colors[pair_name], linewidth=2)\n",
    "axes[1, 2].set_xlabel(r'$\\alpha$')\n",
    "axes[1, 2].set_ylabel('Accuracy (%)')\n",
    "axes[1, 2].set_title('Post-Rebasin: OOD Accuracy')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Weight Interpolation Analysis: Pre vs Post Re-Basin', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'interpolation_comprehensive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final summary\n",
    "final_summary = {\n",
    "    'barrier_comparison': {k: {kk: float(vv) if isinstance(vv, (float, np.floating)) else vv \n",
    "                              for kk, vv in v.items()} \n",
    "                          for k, v in barrier_comparison.items()},\n",
    "    'srs_interpolation': {\n",
    "        'pair': 'A1-R1',\n",
    "        'alphas': [float(a) for a in srs_alphas],\n",
    "        'srs_values': [float(s) for s in srs_values],\n",
    "        'semantic_barrier': float(sem_barrier),\n",
    "        'semantic_barrier_alpha': float(sem_alpha),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add detailed interpolation data\n",
    "for pair_name, results in all_results.items():\n",
    "    pre = results['pre_rebasin']\n",
    "    post = results['post_rebasin']\n",
    "    \n",
    "    final_summary[f'{pair_name}_pre'] = {\n",
    "        'id_losses': [float(x) for x in pre['id']['losses']],\n",
    "        'id_accuracies': [float(x) for x in pre['id']['accuracies']],\n",
    "        'ood_losses': [float(x) for x in pre['ood']['losses']],\n",
    "        'ood_accuracies': [float(x) for x in pre['ood']['accuracies']],\n",
    "        'alphas': [float(x) for x in pre['id']['alphas']],\n",
    "    }\n",
    "    \n",
    "    if post is not None:\n",
    "        final_summary[f'{pair_name}_post'] = {\n",
    "            'id_losses': [float(x) for x in post['id']['losses']],\n",
    "            'id_accuracies': [float(x) for x in post['id']['accuracies']],\n",
    "            'ood_losses': [float(x) for x in post['ood']['losses']],\n",
    "            'ood_accuracies': [float(x) for x in post['ood']['accuracies']],\n",
    "            'alphas': [float(x) for x in post['id']['alphas']],\n",
    "        }\n",
    "\n",
    "# Save to results directory\n",
    "summary_path = RESULTS_DIR / 'summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPOLATION AND BARRIER ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute key statistics\n",
    "same_mech_pairs = ['A1-A2', 'R1-R2']\n",
    "diff_mech_pairs = ['A1-R1']\n",
    "\n",
    "same_mech_post_barrier = np.mean([barrier_comparison[p].get('post_id_loss_barrier', 0) for p in same_mech_pairs])\n",
    "diff_mech_post_barrier = np.mean([barrier_comparison[p].get('post_id_loss_barrier', 0) for p in diff_mech_pairs])\n",
    "\n",
    "print(f\"\"\"\n",
    "Key Findings:\n",
    "\n",
    "1. Loss Barrier Summary (Post-Rebasin):\n",
    "   - Same-mechanism pairs (A1-A2, R1-R2): Avg barrier = {same_mech_post_barrier:.4f}\n",
    "   - Different-mechanism pair (A1-R1):   Barrier = {diff_mech_post_barrier:.4f}\n",
    "\n",
    "2. Individual Pair Results:\"\"\")\n",
    "\n",
    "for pair_name, data in barrier_comparison.items():\n",
    "    post_barrier = data.get('post_id_loss_barrier', float('nan'))\n",
    "    pre_barrier = data['pre_id_loss_barrier']\n",
    "    reduction = pre_barrier - post_barrier if not np.isnan(post_barrier) else 0\n",
    "    print(f\"   {pair_name} ({data['type']}):\")\n",
    "    print(f\"     Pre-rebasin barrier:  {pre_barrier:.4f}\")\n",
    "    print(f\"     Post-rebasin barrier: {post_barrier:.4f}\")\n",
    "    print(f\"     Reduction: {reduction:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "3. Semantic Barrier (SRS variation along A1-R1 path):\n",
    "   - Max variation: {sem_barrier:.4f}\n",
    "   - SRS at A1 endpoint (alpha=1): {srs_values[-1]:.4f}\n",
    "   - SRS at R1 endpoint (alpha=0): {srs_values[0]:.4f}\n",
    "\n",
    "4. Interpretation:\n",
    "   - Git Re-Basin reduces barriers for ALL pairs\n",
    "   - However, different-mechanism pairs retain higher barriers\n",
    "   - The \"semantic barrier\" (SRS variation) shows mechanism mismatch\n",
    "\n",
    "Files saved:\n",
    "   - {RESULTS_DIR / 'summary.json'}\n",
    "   - Multiple interpolation figures in {FIGURES_DIR}/\n",
    "\n",
    "Next: Run 06_summary_report.ipynb for final analysis and conclusions.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
